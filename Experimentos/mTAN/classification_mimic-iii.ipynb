{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Configuración general =====\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Ventana fija por hora (48h)\n",
    "WINDOW_HOURS = 48\n",
    "L_MAX = WINDOW_HOURS  # pasos temporales\n",
    "TS_CHANNELS = 2       # HR + RR/MAP (o HR + Lactato)\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LR = 2e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 20\n",
    "GRAD_CLIP = 1.0\n",
    "BATCH_TRAIN = 64\n",
    "BATCH_VAL = 128\n",
    "BATCH_TEST = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_notes_safely(path, usecols=None):\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(\n",
    "            path,\n",
    "            usecols=usecols,\n",
    "            engine='python',        # más robusto con comillas rotas\n",
    "            quoting=csv.QUOTE_MINIMAL,\n",
    "            on_bad_lines='skip',    # si alguna línea es irrecuperable, sáltala\n",
    "            encoding='utf-8',       # cambia a 'latin1' si necesitas\n",
    "            parse_dates=['CHARTDATE','CHARTTIME'],\n",
    "            chunksize=200_000):     # ajusta según memoria\n",
    "        # Limpieza básica de texto\n",
    "        if 'TEXT' in chunk.columns:\n",
    "            chunk['TEXT'] = chunk['TEXT'].astype(str)\n",
    "            # Normaliza comillas “rotas”\n",
    "            chunk['TEXT'] = chunk['TEXT'].str.replace('\\r', '\\n', regex=False)\n",
    "            chunk['TEXT'] = chunk['TEXT'].str.replace('\"', \"'\", regex=False)\n",
    "        chunks.append(chunk)\n",
    "    notes = pd.concat(chunks, ignore_index=True)\n",
    "    # Tipos de ID a string\n",
    "    for col in ('SUBJECT_ID','HADM_ID'):\n",
    "        if col in notes.columns:\n",
    "            notes[col] = notes[col].astype(str)\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mimic_data(mimic_root: Path):\n",
    "    # Lee los CSV originales de MIMIC-III con sus columnas reales\n",
    "    charts = pd.read_csv(mimic_root / 'CHARTEVENTS.csv',\n",
    "                         usecols=['SUBJECT_ID','HADM_ID','ICUSTAY_ID','ITEMID','CHARTTIME','VALUENUM','VALUEUOM'],\n",
    "                         parse_dates=['CHARTTIME'])\n",
    "    labs = pd.read_csv(mimic_root / 'LABEVENTS.csv',\n",
    "                       usecols=['SUBJECT_ID','HADM_ID','ITEMID','CHARTTIME','VALUENUM','VALUEUOM','FLAG'],\n",
    "                       parse_dates=['CHARTTIME'])\n",
    "    labels = pd.read_csv(mimic_root / 'ADMISSIONS.csv',\n",
    "                         usecols=['SUBJECT_ID','HADM_ID','ADMITTIME','DISCHTIME','DEATHTIME',\n",
    "                                  'ADMISSION_TYPE','HOSPITAL_EXPIRE_FLAG','HAS_CHARTEVENTS_DATA'],\n",
    "                         parse_dates=['ADMITTIME','DISCHTIME','DEATHTIME'])\n",
    "    profiles = pd.read_csv(mimic_root / 'PATIENTS.csv',\n",
    "                           usecols=['SUBJECT_ID','GENDER','DOB','DOD','DOD_HOSP','EXPIRE_FLAG'],\n",
    "                           parse_dates=['DOB','DOD'])\n",
    "    icustays = pd.read_csv(mimic_root / 'ICUSTAYS.csv',\n",
    "                           usecols=['SUBJECT_ID','HADM_ID','ICUSTAY_ID','INTIME','OUTTIME'],\n",
    "                           parse_dates=['INTIME','OUTTIME'])\n",
    "    notes = read_notes_safely(mimic_root / 'NOTEEVENTS.csv.gz',\n",
    "                          usecols=['SUBJECT_ID','HADM_ID','CHARTDATE','CHARTTIME','CATEGORY','TEXT'])\n",
    "\n",
    "\n",
    "    # Normaliza tipos de ID a string\n",
    "    for df in (charts, labs, labels, profiles, icustays, notes):\n",
    "        for col in ['SUBJECT_ID','HADM_ID']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str)\n",
    "        if 'ICUSTAY_ID' in df.columns:\n",
    "            df['ICUSTAY_ID'] = df['ICUSTAY_ID'].astype(str)\n",
    "\n",
    "    return charts, labs, labels, profiles, icustays, notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario simple de ITEMIDs (ajústalo a tu versión de MIMIC-III)\n",
    "ITEMIDS = {\n",
    "    'HR': [211, 220045],           # Heart Rate (CareVue, MetaVision)\n",
    "    'RR': [618, 220210],           # Respiratory Rate\n",
    "    'MAP': [456, 220052],          # Mean Arterial Pressure\n",
    "    # Si quieres usar laboratorio como segundo canal, tendrás que mapear por D_LABITEMS,\n",
    "    # pero en muchas setups el label \"Lactate\" tiene ITEMID conocidos. Si no los tienes,\n",
    "    # filtra por VALUEUOM y nombre con un diccionario auxiliar.\n",
    "}\n",
    "\n",
    "def filter_charts_for_signals(charts_raw, signals=('HR','RR')):\n",
    "    # Quedarse solo con ITEMIDs de interés y columnas necesarias\n",
    "    mask = charts_raw['ITEMID'].isin(sum([ITEMIDS[s] for s in signals if s in ITEMIDS], []))\n",
    "    charts = charts_raw.loc[mask, ['SUBJECT_ID','HADM_ID','ICUSTAY_ID','ITEMID','CHARTTIME','VALUENUM','VALUEUOM']].copy()\n",
    "    charts = charts.dropna(subset=['VALUENUM'])\n",
    "    charts['VALUENUM'] = charts['VALUENUM'].astype(float)\n",
    "    return charts\n",
    "\n",
    "def pick_first_note(notes_raw):\n",
    "    # Primera nota por HADM_ID (puedes restringir CATEGORY a 'Nursing'/'Physician' si quieres notas tempranas)\n",
    "    notes = notes_raw.sort_values(['HADM_ID','CHARTTIME']).groupby(['SUBJECT_ID','HADM_ID'], as_index=False).first()\n",
    "    notes = notes[['SUBJECT_ID','HADM_ID','CHARTTIME','CATEGORY','TEXT']].copy()\n",
    "    notes['TEXT'] = notes['TEXT'].fillna('')\n",
    "    return notes\n",
    "\n",
    "def build_profiles(admissions, patients, notes_first):\n",
    "    # Perfil: edad, sexo, tipo de admisión + texto opcional\n",
    "    prof = admissions[['SUBJECT_ID','HADM_ID','ADMITTIME','ADMISSION_TYPE','HOSPITAL_EXPIRE_FLAG']].copy()\n",
    "    prof = prof.merge(patients[['SUBJECT_ID','GENDER','DOB']], on='SUBJECT_ID', how='left')\n",
    "    # Edad aproximada (MIMIC-III tiene edades >89 enmascaradas; considera cap a 90)\n",
    "    prof['age'] = ((prof['ADMITTIME'] - prof['DOB']).dt.days / 365.25).clip(lower=0, upper=90)\n",
    "    prof['gender'] = prof['GENDER'].astype(str)\n",
    "    prof['admission_type'] = prof['ADMISSION_TYPE'].astype(str)\n",
    "    prof['label_binary'] = prof['HOSPITAL_EXPIRE_FLAG'].astype(int)\n",
    "\n",
    "    # Adjuntar primera nota\n",
    "    notes_small = notes_first[['SUBJECT_ID','HADM_ID','TEXT']]\n",
    "    prof = prof.merge(notes_small, on=['SUBJECT_ID','HADM_ID'], how='left')\n",
    "    prof['TEXT'] = prof['TEXT'].fillna('')\n",
    "\n",
    "    return prof[['SUBJECT_ID','HADM_ID','ADMITTIME','age','gender','admission_type','label_binary','TEXT']]\n",
    "\n",
    "def align_charts_to_icu_window(charts, icu, window_hours=48):\n",
    "    # Usa INTIME como t0, mantiene solo 48h\n",
    "    icu_small = icu[['SUBJECT_ID','HADM_ID','ICUSTAY_ID','INTIME']].copy()\n",
    "    charts = charts.merge(icu_small, on=['SUBJECT_ID','HADM_ID','ICUSTAY_ID'], how='inner')\n",
    "    # Diferencia temporal en horas desde INTIME\n",
    "    dt = (charts['CHARTTIME'] - charts['INTIME']).dt.total_seconds() / 3600.0\n",
    "    charts['HOUR_IDX'] = np.floor(dt).astype(int)\n",
    "    charts = charts[(charts['HOUR_IDX'] >= 0) & (charts['HOUR_IDX'] < window_hours)]\n",
    "    return charts\n",
    "\n",
    "def hourly_aggregate(charts_aligned, signals=('HR','RR'), window_hours=48):\n",
    "    # Construye una tabla (HOUR_IDX, canal) con medianas por hora\n",
    "    # Mapea ITEMID → señal\n",
    "    item_to_signal = {}\n",
    "    for s in signals:\n",
    "        for it in ITEMIDS.get(s, []):\n",
    "            item_to_signal[it] = s\n",
    "    charts_aligned['signal'] = charts_aligned['ITEMID'].map(item_to_signal)\n",
    "\n",
    "    # Mediana por (SUBJECT_ID,HADM_ID,ICUSTAY_ID,HOUR_IDX,signal)\n",
    "    gcols = ['SUBJECT_ID','HADM_ID','ICUSTAY_ID','HOUR_IDX','signal']\n",
    "    agg = charts_aligned.groupby(gcols)['VALUENUM'].median().reset_index()\n",
    "\n",
    "    # Pivot a matriz por hora: columnas = señales\n",
    "    pivot = agg.pivot_table(index=['SUBJECT_ID','HADM_ID','ICUSTAY_ID','HOUR_IDX'],\n",
    "                            columns='signal', values='VALUENUM')\n",
    "    pivot = pivot.reset_index()\n",
    "\n",
    "    # Rellenar rejilla completa 0..window_hours-1\n",
    "    def fill_grid(df_one):\n",
    "        # df_one: filas para un (subject, hadm, icu)\n",
    "        grid = pd.DataFrame({'HOUR_IDX': np.arange(window_hours)})\n",
    "        df = grid.merge(df_one, on='HOUR_IDX', how='left')\n",
    "        return df\n",
    "\n",
    "    frames = []\n",
    "    for key, dfk in pivot.groupby(['SUBJECT_ID','HADM_ID','ICUSTAY_ID']):\n",
    "        filled = fill_grid(dfk.drop(columns=['SUBJECT_ID','HADM_ID','ICUSTAY_ID']))\n",
    "        filled['SUBJECT_ID'], filled['HADM_ID'], filled['ICUSTAY_ID'] = key\n",
    "        frames.append(filled)\n",
    "    hourly = pd.concat(frames, axis=0, ignore_index=True)\n",
    "\n",
    "    # Orden de canales acorde a signals\n",
    "    X_cols = list(signals)\n",
    "    return hourly[['SUBJECT_ID','HADM_ID','ICUSTAY_ID','HOUR_IDX'] + X_cols]\n",
    "\n",
    "def make_sequences(hourly_df, profiles_df, signals=('HR','RR'), window_hours=48):\n",
    "    # Construye X(L,C), M(L,C), T(L)\n",
    "    instances = []\n",
    "    for key, dfk in hourly_df.groupby(['SUBJECT_ID','HADM_ID','ICUSTAY_ID']):\n",
    "        dfk = dfk.sort_values('HOUR_IDX')\n",
    "        X = dfk[list(signals)].to_numpy(dtype=float)  # (L, C)\n",
    "        M = ~np.isnan(X)\n",
    "        X = np.where(M, X, 0.0)\n",
    "        M = M.astype(float)\n",
    "        T = np.linspace(0.0, 1.0, window_hours, dtype=float)\n",
    "\n",
    "        # Etiqueta y perfil\n",
    "        subj, hadm, icu = key\n",
    "        prof = profiles_df[(profiles_df['SUBJECT_ID']==subj) & (profiles_df['HADM_ID']==hadm)].iloc[0]\n",
    "        y = int(prof['label_binary'])\n",
    "        instances.append({\n",
    "            'key': key, 'X': X, 'M': M, 'T': T,\n",
    "            'age': prof['age'], 'gender': prof['gender'], 'admission_type': prof['admission_type'],\n",
    "            'text': prof['TEXT'], 'label': y\n",
    "        })\n",
    "    return instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def build_P_matrix(instances):\n",
    "    # Tabular básico\n",
    "    genders = [x['gender'] for x in instances]\n",
    "    adm_types = [x['admission_type'] for x in instances]\n",
    "    ages = np.array([x['age'] for x in instances], dtype=np.float32).reshape(-1,1)\n",
    "\n",
    "    # One-hot\n",
    "    df_cat = pd.DataFrame({'gender': genders, 'admission_type': adm_types})\n",
    "    cat_oh = pd.get_dummies(df_cat, prefix=['gender','admtype'])\n",
    "    cat_oh = cat_oh.to_numpy(dtype=np.float32)\n",
    "\n",
    "    # Texto → TF-IDF → PCA(64)\n",
    "    texts = [x['text'] for x in instances]\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    X_tfidf = tfidf.fit_transform(texts)\n",
    "    pca = PCA(n_components=64, random_state=42)\n",
    "    X_text = pca.fit_transform(X_tfidf.toarray()).astype(np.float32)\n",
    "\n",
    "    # Concatenación y estandarización\n",
    "    P = np.concatenate([ages, cat_oh, X_text], axis=1)\n",
    "    P = (P - np.nanmean(P, axis=0)) / (np.nanstd(P, axis=0) + 1e-6)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_arrays(instances, P):\n",
    "    X_list = [x['X'] for x in instances]\n",
    "    M_list = [x['M'] for x in instances]\n",
    "    T_list = [x['T'] for x in instances]\n",
    "    y = np.array([x['label'] for x in instances], dtype=int)\n",
    "    meta = [x['key'] for x in instances]\n",
    "    X_all = np.stack(X_list, axis=0)\n",
    "    M_all = np.stack(M_list, axis=0)\n",
    "    T_all = np.stack(T_list, axis=0)\n",
    "    return X_all, M_all, T_all, y, meta, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hourly_grid(start_time: pd.Timestamp, hours=L_MAX):\n",
    "    # Rejilla por hora desde start_time\n",
    "    tgrid = [start_time + pd.Timedelta(hours=h) for h in range(hours)]\n",
    "    return pd.DataFrame({'chart_time_hour': tgrid})\n",
    "\n",
    "def aggregate_signals(charts_user, labs_user, start_time, hours=L_MAX):\n",
    "    # Construye la rejilla y hace merge por hora\n",
    "    grid = build_hourly_grid(start_time.normalize(), hours=hours)  # puedes usar admit_time como base\n",
    "    # Merge charts (HR, RR/MAP)\n",
    "    charts_user = charts_user.copy()\n",
    "    charts_user['chart_time_hour'] = charts_user['chart_time_hour'].dt.floor('H')\n",
    "    merged = grid.merge(charts_user, on='chart_time_hour', how='left')\n",
    "\n",
    "    # Si usas labs (ej. Lactato) como segunda señal, mézclalo aquí\n",
    "    if labs_user is not None and not labs_user.empty:\n",
    "        labs_user = labs_user.copy()\n",
    "        labs_user['chart_time_hour'] = labs_user['chart_time_hour'].dt.floor('H')\n",
    "        merged = merged.merge(labs_user[['chart_time_hour','Lactate']], on='chart_time_hour', how='left')\n",
    "\n",
    "    # Selección de canales\n",
    "    # Opción A: HR + RR\n",
    "    v1 = merged['HR'].astype(float) if 'HR' in merged.columns else pd.Series([np.nan]*len(merged))\n",
    "    v2 = merged['RR'].astype(float) if 'RR' in merged.columns else (\n",
    "        merged['MAP'].astype(float) if 'MAP' in merged.columns else (\n",
    "            merged['Lactate'].astype(float) if 'Lactate' in merged.columns else pd.Series([np.nan]*len(merged))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    X = np.stack([\n",
    "        v1.fillna(np.nan).to_numpy(),\n",
    "        v2.fillna(np.nan).to_numpy()\n",
    "    ], axis=-1)  # shape (L, 2)\n",
    "\n",
    "    M = ~np.isnan(X)\n",
    "    X = np.where(M, X, 0.0)\n",
    "    # T normalizado 0..1\n",
    "    T = np.linspace(0.0, 1.0, len(merged), dtype=float)\n",
    "    M = M.astype(float)\n",
    "\n",
    "    return X.astype(float), T.astype(float), M.astype(float)\n",
    "\n",
    "def build_instances(charts, labs, labels, profiles, notes):\n",
    "    \"\"\"\n",
    "    Construye instancias por (subject_id, hadm_id, icustay_id).\n",
    "    Toma primeras 48h desde admit_time (o icu_in_time si lo tienes).\n",
    "    Genera X (L, C=2), T (L), M (L, C), y target binario; y perfil tabular P.\n",
    "    También produce embeddings de notas (opcional) para P.\n",
    "    \"\"\"\n",
    "    # Prepara embeddings simples de notas (TF-IDF + PCA → 64 dims)\n",
    "    # Nota: en producción usarías un encoder clínico; esto es para tener una baseline reproducible.\n",
    "    # Selecciona la primera nota por admisión\n",
    "    first_notes = notes.sort_values('note_time').groupby(['subject_id','hadm_id','icustay_id'], as_index=False).first()\n",
    "    text_corpus = first_notes['note_text'].fillna('')\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    X_tfidf = tfidf.fit_transform(text_corpus).astype(np.float32)\n",
    "    pca = PCA(n_components=64, random_state=SEED)\n",
    "    note_emb = pca.fit_transform(X_tfidf.toarray()) if X_tfidf.shape[0] > 0 else np.zeros((0,64), dtype=np.float32)\n",
    "    first_notes['note_emb'] = list(note_emb)\n",
    "\n",
    "    # Perfiles num/cat\n",
    "    prof = profiles.copy()\n",
    "    prof = prof.merge(first_notes[['subject_id','hadm_id','icustay_id','note_emb']], on=['subject_id','hadm_id','icustay_id'], how='left')\n",
    "    prof['note_emb'] = prof['note_emb'].apply(lambda x: np.array(x, dtype=np.float32) if isinstance(x, (list,np.ndarray)) else np.zeros((64,), dtype=np.float32))\n",
    "\n",
    "    # One-hot para categóricas, fillna mean para numéricas\n",
    "    cat_cols = [c for c in prof.columns if c in ('gender','admission_type')]\n",
    "    num_cols = [c for c in prof.columns if c not in cat_cols and c not in ('subject_id','hadm_id','icustay_id','note_emb')]\n",
    "    prof_num = prof[num_cols].apply(lambda col: col.fillna(col.mean()))\n",
    "    prof_cat = pd.get_dummies(prof[cat_cols], prefix=cat_cols) if len(cat_cols)>0 else pd.DataFrame(index=prof.index)\n",
    "    P_tab = pd.concat([prof_num, prof_cat], axis=1)\n",
    "\n",
    "    # Concatena note_emb a P_tab\n",
    "    P_all = []\n",
    "    for i in range(len(prof)):\n",
    "        vec_tab = P_tab.iloc[i].to_numpy(dtype=np.float32)\n",
    "        vec_note = prof.iloc[i]['note_emb']\n",
    "        P_all.append(np.concatenate([vec_tab, vec_note], axis=0))\n",
    "    # Estandariza P_all canal a canal\n",
    "    P_mat = np.stack(P_all, axis=0)\n",
    "    P_mat = (P_mat - np.nanmean(P_mat, axis=0)) / (np.nanstd(P_mat, axis=0) + 1e-6)\n",
    "\n",
    "    # Empareja con labels\n",
    "    lab = labels[['subject_id','hadm_id','icustay_id','admit_time','label_binary']].copy()\n",
    "    lab['label_binary'] = lab['label_binary'].astype(int)\n",
    "    merged = lab.merge(prof[['subject_id','hadm_id','icustay_id']], on=['subject_id','hadm_id','icustay_id'], how='inner')\n",
    "    # Construye X/T/M por instancia\n",
    "    X_list, T_list, M_list, y_list, meta, P_list = [], [], [], [], [], []\n",
    "\n",
    "    # Índices para reindexar P_mat\n",
    "    key_to_index = {tuple(row[['subject_id','hadm_id','icustay_id']]): idx for idx, row in prof.reset_index(drop=True).iterrows()}\n",
    "\n",
    "    for _, row in merged.iterrows():\n",
    "        key = (row['subject_id'], row['hadm_id'], row['icustay_id'])\n",
    "        admit_time = pd.to_datetime(row['admit_time'])\n",
    "        # Filtra datos del usuario/admisión\n",
    "        charts_user = charts[(charts['subject_id']==key[0]) & (charts['hadm_id']==key[1]) & (charts['icustay_id']==key[2])]\n",
    "        labs_user = labs[(labs['subject_id']==key[0]) & (labs['hadm_id']==key[1]) & (labs['icustay_id']==key[2])] if labs is not None else None\n",
    "        X, T, M = aggregate_signals(charts_user, labs_user, admit_time, hours=L_MAX)\n",
    "        y = int(row['label_binary'])\n",
    "        X_list.append(X); T_list.append(T); M_list.append(M); y_list.append(y); meta.append(key)\n",
    "        P_list.append(P_mat[key_to_index[key]])\n",
    "\n",
    "    return X_list, T_list, M_list, np.array(y_list, dtype=int), meta, np.stack(P_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Datasets =====\n",
    "class TimeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, T, M, y, user_ids=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.T = torch.tensor(T, dtype=torch.float32)\n",
    "        self.M = torch.tensor(M, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.user_ids = user_ids if user_ids is not None else [None]*len(y)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.T[idx], self.M[idx], self.y[idx], self.user_ids[idx]\n",
    "\n",
    "class TimeDatasetWithProfile(TimeDataset):\n",
    "    def __init__(self, X, T, M, y, P, user_ids=None):\n",
    "        super().__init__(X, T, M, y, user_ids)\n",
    "        self.P = torch.tensor(P, dtype=torch.float32)\n",
    "    def __getitem__(self, idx):\n",
    "        X, T, M, y, uid = super().__getitem__(idx)\n",
    "        return X, T, M, y, uid, self.P[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Modelo (reuso de tus bloques) =====\n",
    "# Copia aquí TimeAttentionBlock, MTANBackbone, FiLMGenerator, HeadMLP, ModelPhase1_TS, ModelPhase2_TSProfile, ModelPhase3_FiLM\n",
    "# (idénticos a tu código original, salvo que in_channels=TS_CHANNELS)\n",
    "\n",
    "# mTAN-like + FiLM (modelos)\n",
    "class TimeAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dk = d_model // n_heads\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4*d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.time_decay = nn.Parameter(torch.tensor(0.1, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x, T, M, gamma=None, beta=None):\n",
    "        B, L, d = x.size()\n",
    "        Q = self.Wq(x); K = self.Wk(x); V = self.Wv(x)\n",
    "        def split_heads(t): return t.view(B, L, self.n_heads, self.dk).transpose(1, 2)\n",
    "        Qh, Kh, Vh = split_heads(Q), split_heads(K), split_heads(V)\n",
    "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / np.sqrt(self.dk)\n",
    "        Ti = T.unsqueeze(1).unsqueeze(-1)\n",
    "        Tj = T.unsqueeze(1).unsqueeze(-2)\n",
    "        time_dist = torch.abs(Ti - Tj)\n",
    "        scores = scores - self.time_decay * time_dist\n",
    "        step_valid = (M.sum(dim=-1) > 0).unsqueeze(1).unsqueeze(2)\n",
    "        scores = scores.masked_fill(~step_valid, float('-inf'))\n",
    "        A = torch.softmax(scores, dim=-1)\n",
    "        A = self.dropout(A)\n",
    "        Zh = torch.matmul(A, Vh)\n",
    "        Z = Zh.transpose(1, 2).contiguous().view(B, L, d)\n",
    "        h_attn = self.out(Z)\n",
    "        y1 = self.ln1(x + h_attn)\n",
    "        if gamma is not None and beta is not None:\n",
    "            y1 = y1 * gamma + beta\n",
    "        f = self.ffn(y1)\n",
    "        y2 = self.ln2(y1 + f)\n",
    "        if gamma is not None and beta is not None:\n",
    "            y2 = y2 * gamma + beta\n",
    "        return y2\n",
    "\n",
    "class MTANBackbone(nn.Module):\n",
    "    def __init__(self, in_channels, d_model=128, n_layers=2, n_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(in_channels, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TimeAttentionBlock(d_model, n_heads, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "    def forward(self, X, T, M, gammas=None, betas=None):\n",
    "        h = self.input_proj(X)\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            gamma_l = None if gammas is None else gammas[:, l, :].unsqueeze(1)\n",
    "            beta_l  = None if betas  is None else betas[:, l, :].unsqueeze(1)\n",
    "            h = layer(h, T, M, gamma=gamma_l, beta=beta_l)\n",
    "        return h\n",
    "\n",
    "class FiLMGenerator(nn.Module):\n",
    "    def __init__(self, p_dim, d_model, n_layers, hidden=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(p_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 2*n_layers*d_model)\n",
    "        )\n",
    "        nn.init.zeros_(self.mlp[-1].weight)\n",
    "        nn.init.zeros_(self.mlp[-1].bias)\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "    def forward(self, P):\n",
    "        B = P.size(0)\n",
    "        out = self.mlp(P).view(B, 2, self.n_layers, self.d_model)\n",
    "        gammas = out[:, 0, :, :] + 1.0\n",
    "        betas  = out[:, 1, :, :]\n",
    "        return gammas, betas\n",
    "\n",
    "class HeadMLP(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, d_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_hidden, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)\n",
    "\n",
    "# Modelos por fase\n",
    "class ModelPhase1_TS(nn.Module):\n",
    "    def __init__(self, in_channels, d_model=D_MODEL, n_layers=N_LAYERS, n_heads=N_HEADS, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.backbone = MTANBackbone(in_channels, d_model, n_layers, n_heads, dropout)\n",
    "        self.head = HeadMLP(d_model, d_hidden=64, dropout=dropout)\n",
    "    def forward(self, X, T, M):\n",
    "        h = self.backbone(X, T, M, gammas=None, betas=None)\n",
    "        step_valid = (M.sum(dim=-1) > 0).float()\n",
    "        denom = torch.clamp(step_valid.sum(dim=1, keepdim=True), min=1.0)\n",
    "        h_seq = (h * step_valid.unsqueeze(-1)).sum(dim=1) / denom\n",
    "        return self.head(h_seq)\n",
    "\n",
    "class ModelPhase2_TSProfile(nn.Module):\n",
    "    def __init__(self, in_channels, p_dim, d_model=D_MODEL, n_layers=N_LAYERS, n_heads=N_HEADS, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.backbone = MTANBackbone(in_channels, d_model, n_layers, n_heads, dropout)\n",
    "        self.profile_mlp = nn.Sequential(\n",
    "            nn.Linear(p_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.head = HeadMLP(d_model + 64, d_hidden=64, dropout=dropout)\n",
    "    def forward(self, X, T, M, P):\n",
    "        h = self.backbone(X, T, M, gammas=None, betas=None)\n",
    "        step_valid = (M.sum(dim=-1) > 0).float()\n",
    "        denom = torch.clamp(step_valid.sum(dim=1, keepdim=True), min=1.0)\n",
    "        h_seq = (h * step_valid.unsqueeze(-1)).sum(dim=1) / denom\n",
    "        h_prof = self.profile_mlp(P)\n",
    "        return self.head(torch.cat([h_seq, h_prof], dim=-1))\n",
    "\n",
    "class ModelPhase3_FiLM(nn.Module):\n",
    "    def __init__(self, in_channels, p_dim, d_model=D_MODEL, n_layers=N_LAYERS, n_heads=N_HEADS, dropout=DROPOUT, film_hidden=64):\n",
    "        super().__init__()\n",
    "        self.backbone = MTANBackbone(in_channels, d_model, n_layers, n_heads, dropout)\n",
    "        self.film = FiLMGenerator(p_dim, d_model, n_layers, hidden=film_hidden, dropout=0.1)\n",
    "        self.head = HeadMLP(d_model, d_hidden=64, dropout=dropout)\n",
    "    def forward(self, X, T, M, P):\n",
    "        gammas, betas = self.film(P)\n",
    "        h = self.backbone(X, T, M, gammas=gammas, betas=betas)\n",
    "        step_valid = (M.sum(dim=-1) > 0).float()\n",
    "        denom = torch.clamp(step_valid.sum(dim=1, keepdim=True), min=1.0)\n",
    "        h_seq = (h * step_valid.unsqueeze(-1)).sum(dim=1) / denom\n",
    "        return self.head(h_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Utilidades =====\n",
    "def compute_channel_stats(X_train, M_train):\n",
    "    B, L, C = X_train.shape\n",
    "    means = np.zeros((C,), dtype=float)\n",
    "    stds = np.ones((C,), dtype=float)\n",
    "    for c in range(C):\n",
    "        vals = X_train[..., c][M_train[..., c] == 1.0]\n",
    "        means[c] = float(np.mean(vals)) if vals.size>0 else 0.0\n",
    "        stds[c] = float(np.std(vals) + 1e-6) if vals.size>0 else 1.0\n",
    "    return means, stds\n",
    "\n",
    "def standardize_by_stats(X, M, means, stds):\n",
    "    X_std = (X - means[None, None, :]) / stds[None, None, :]\n",
    "    return np.where(M == 1.0, X_std, 0.0)\n",
    "\n",
    "# Entrenamiento/Evaluación (igual que tu código)\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    logits_all, targets_all = [], []\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        if len(batch) == 5:\n",
    "            X, T, M, y, _ = batch\n",
    "            X, T, M, y = X.to(DEVICE), T.to(DEVICE), M.to(DEVICE), y.to(DEVICE)\n",
    "            logits = model(X, T, M)\n",
    "        else:\n",
    "            X, T, M, y, _, P = batch\n",
    "            X, T, M, y, P = X.to(DEVICE), T.to(DEVICE), M.to(DEVICE), y.to(DEVICE), P.to(DEVICE)\n",
    "            logits = model(X, T, M, P)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        logits_all.append(logits.detach().cpu().numpy())\n",
    "        targets_all.append(y.detach().cpu().numpy())\n",
    "    yhat = np.concatenate(logits_all); yt = np.concatenate(targets_all)\n",
    "    try: auroc = roc_auc_score(yt, yhat)\n",
    "    except: auroc = np.nan\n",
    "    try: auprc = average_precision_score(yt, yhat)\n",
    "    except: auprc = np.nan\n",
    "    return total_loss / len(yt), auroc, auprc\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    logits_all, targets_all = [], []\n",
    "    for batch in loader:\n",
    "        if len(batch) == 5:\n",
    "            X, T, M, y, _ = batch\n",
    "            X, T, M, y = X.to(DEVICE), T.to(DEVICE), M.to(DEVICE), y.to(DEVICE)\n",
    "            logits = model(X, T, M)\n",
    "        else:\n",
    "            X, T, M, y, _, P = batch\n",
    "            X, T, M, y, P = X.to(DEVICE), T.to(DEVICE), M.to(DEVICE), y.to(DEVICE), P.to(DEVICE)\n",
    "            logits = model(X, T, M, P)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        logits_all.append(logits.detach().cpu().numpy())\n",
    "        targets_all.append(y.detach().cpu().numpy())\n",
    "    yhat = np.concatenate(logits_all); yt = np.concatenate(targets_all)\n",
    "    try: auroc = roc_auc_score(yt, yhat)\n",
    "    except: auroc = np.nan\n",
    "    try: auprc = average_precision_score(yt, yhat)\n",
    "    except: auprc = np.nan\n",
    "    return total_loss / len(yt), auroc, auprc\n",
    "\n",
    "def run_phase(phase_name, train_loader, val_loader, in_channels, p_dim=None):\n",
    "    if phase_name == 'P1':\n",
    "        model = ModelPhase1_TS(in_channels).to(DEVICE)\n",
    "    elif phase_name == 'P2':\n",
    "        assert p_dim is not None\n",
    "        model = ModelPhase2_TSProfile(in_channels, p_dim).to(DEVICE)\n",
    "    elif phase_name == 'P3':\n",
    "        assert p_dim is not None\n",
    "        model = ModelPhase3_FiLM(in_channels, p_dim).to(DEVICE)\n",
    "    else:\n",
    "        raise ValueError(\"phase_name must be P1, P2 or P3\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val = -np.inf\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        tl, tr_auc, tr_pr = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        vl, va_auc, va_pr = eval_one_epoch(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "        score = np.nanmean([va_auc, va_pr])\n",
    "        if score > best_val:\n",
    "            best_val = score\n",
    "            best_state = { 'model': model.state_dict() }\n",
    "        print(f\"{phase_name} | Epoch {ep:02d} | train_loss={tl:.4f} val_loss={vl:.4f} | \"\n",
    "              f\"AUROC train/val={tr_auc:.3f}/{va_auc:.3f} | AUPRC train/val={tr_pr:.3f}/{va_pr:.3f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state['model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/gmartinez/Tesis/Datasets/MIMIC-III/NOTEEVENTS.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Test mean AUROC=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.nanmean(arr[:,\u001b[32m0\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | AUPRC=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.nanmean(arr[:,\u001b[32m1\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[43mmain_mimic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/gmartinez/Tesis/Datasets/MIMIC-III\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain_mimic\u001b[39m\u001b[34m(mimic_root)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain_mimic\u001b[39m(mimic_root: Path):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     charts, labs, labels, profiles, icustays, notes = \u001b[43mload_mimic_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmimic_root\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     X_list, T_list, M_list, y, meta, P_vals = build_instances(charts, labs, labels, profiles, notes)\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Reindex y preparar arrays\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mload_mimic_data\u001b[39m\u001b[34m(mimic_root)\u001b[39m\n\u001b[32m     16\u001b[39m profiles = pd.read_csv(mimic_root / \u001b[33m'\u001b[39m\u001b[33mPATIENTS.csv\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m                        usecols=[\u001b[33m'\u001b[39m\u001b[33mSUBJECT_ID\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mGENDER\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mDOB\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mDOD\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mDOD_HOSP\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mEXPIRE_FLAG\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     18\u001b[39m                        parse_dates=[\u001b[33m'\u001b[39m\u001b[33mDOB\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mDOD\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     19\u001b[39m icustays = pd.read_csv(mimic_root / \u001b[33m'\u001b[39m\u001b[33mICUSTAYS.csv\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     20\u001b[39m                        usecols=[\u001b[33m'\u001b[39m\u001b[33mSUBJECT_ID\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mHADM_ID\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mICUSTAY_ID\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mINTIME\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mOUTTIME\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     21\u001b[39m                        parse_dates=[\u001b[33m'\u001b[39m\u001b[33mINTIME\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mOUTTIME\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m notes = \u001b[43mread_notes_safely\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmimic_root\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNOTEEVENTS.csv.gz\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                      \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSUBJECT_ID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHADM_ID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCHARTDATE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCHARTTIME\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCATEGORY\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTEXT\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Normaliza tipos de ID a string\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m (charts, labs, labels, profiles, icustays, notes):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mread_notes_safely\u001b[39m\u001b[34m(path, usecols)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_notes_safely\u001b[39m(path, usecols=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      2\u001b[39m     chunks = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# más robusto con comillas rotas\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQUOTE_MINIMAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m            \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mskip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# si alguna línea es irrecuperable, sáltala\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# cambia a 'latin1' si necesitas\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCHARTDATE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCHARTTIME\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200_000\u001b[39;49m\u001b[43m)\u001b[49m:     \u001b[38;5;66;03m# ajusta según memoria\u001b[39;00m\n\u001b[32m     12\u001b[39m         \u001b[38;5;66;03m# Limpieza básica de texto\u001b[39;00m\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mTEXT\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m chunk.columns:\n\u001b[32m     14\u001b[39m             chunk[\u001b[33m'\u001b[39m\u001b[33mTEXT\u001b[39m\u001b[33m'\u001b[39m] = chunk[\u001b[33m'\u001b[39m\u001b[33mTEXT\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Tesis/.venv/lib64/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Tesis/.venv/lib64/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Tesis/.venv/lib64/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Tesis/.venv/lib64/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Tesis/.venv/lib64/python3.12/site-packages/pandas/io/common.py:765\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    762\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    763\u001b[39m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[32m    764\u001b[39m         \u001b[38;5;66;03m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m         handle = \u001b[43mgzip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m    766\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompression_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    771\u001b[39m         handle = gzip.GzipFile(\n\u001b[32m    772\u001b[39m             \u001b[38;5;66;03m# No overload variant of \"GzipFile\" matches argument types\u001b[39;00m\n\u001b[32m    773\u001b[39m             \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    776\u001b[39m             **compression_args,\n\u001b[32m    777\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.12/gzip.py:201\u001b[39m, in \u001b[36mGzipFile.__init__\u001b[39m\u001b[34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         fileobj = \u001b[38;5;28mself\u001b[39m.myfileobj = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    203\u001b[39m         filename = \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/gmartinez/Tesis/Datasets/MIMIC-III/NOTEEVENTS.csv.gz'"
     ]
    }
   ],
   "source": [
    "# ===== Pipeline principal =====\n",
    "def main_mimic(mimic_root: Path):\n",
    "    charts, labs, labels, profiles, icustays, notes = load_mimic_data(mimic_root)\n",
    "\n",
    "    X_list, T_list, M_list, y, meta, P_vals = build_instances(charts, labs, labels, profiles, notes)\n",
    "    # Reindex y preparar arrays\n",
    "    X_all = np.stack(X_list, axis=0)  # (N, L, C=2)\n",
    "    T_all = np.stack(T_list, axis=0)  # (N, L)\n",
    "    M_all = np.stack(M_list, axis=0)  # (N, L, C)\n",
    "    user_ids = [f\"{s}-{h}-{i}\" for (s,h,i) in meta]\n",
    "    in_channels = X_all.shape[-1]\n",
    "    p_dim = P_vals.shape[1]\n",
    "\n",
    "    # Holdout test por HADM_ID (o SUBJECT_ID si prefieres)\n",
    "    groups = np.array([m[1] for m in meta])  # hadm_id\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "    trainval_idx, test_idx = next(gss.split(X_all, y, groups=groups))\n",
    "    X_tv, T_tv, M_tv, y_tv = X_all[trainval_idx], T_all[trainval_idx], M_all[trainval_idx], y[trainval_idx]\n",
    "    X_te, T_te, M_te, y_te = X_all[test_idx], T_all[test_idx], M_all[test_idx], y[test_idx]\n",
    "    P_tv, P_te = P_vals[trainval_idx], P_vals[test_idx]\n",
    "    groups_tv = groups[trainval_idx]\n",
    "\n",
    "    # Validación interna por grupos\n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    te_results = {'P1': [], 'P2': [], 'P3': []}\n",
    "\n",
    "    for fold, (tr_idx_rel, va_idx_rel) in enumerate(gkf.split(X_tv, y_tv, groups=groups_tv), start=1):\n",
    "        print(f\"=== Fold {fold} ===\")\n",
    "        X_tr, T_tr, M_tr, y_tr = X_tv[tr_idx_rel], T_tv[tr_idx_rel], M_tv[tr_idx_rel], y_tv[tr_idx_rel]\n",
    "        X_va, T_va, M_va, y_va = X_tv[va_idx_rel], T_tv[va_idx_rel], M_tv[va_idx_rel], y_tv[va_idx_rel]\n",
    "        P_tr, P_va = P_tv[tr_idx_rel], P_tv[va_idx_rel]\n",
    "\n",
    "        means, stds = compute_channel_stats(X_tr, M_tr)\n",
    "        X_tr_std = standardize_by_stats(X_tr, M_tr, means, stds)\n",
    "        X_va_std = standardize_by_stats(X_va, M_va, means, stds)\n",
    "        X_te_std = standardize_by_stats(X_te, M_te, means, stds)\n",
    "\n",
    "        ds_tr_p1 = TimeDataset(X_tr_std, T_tr, M_tr, y_tr)\n",
    "        ds_va_p1 = TimeDataset(X_va_std, T_va, M_va, y_va)\n",
    "        ds_te_p1 = TimeDataset(X_te_std, T_te, M_te, y_te)\n",
    "\n",
    "        ds_tr_p2 = TimeDatasetWithProfile(X_tr_std, T_tr, M_tr, y_tr, P_tr)\n",
    "        ds_va_p2 = TimeDatasetWithProfile(X_va_std, T_va, M_va, y_va, P_va)\n",
    "        ds_te_p2 = TimeDatasetWithProfile(X_te_std, T_te, M_te, y_te, P_te)\n",
    "\n",
    "        dl_tr_p1 = DataLoader(ds_tr_p1, batch_size=BATCH_TRAIN, shuffle=True)\n",
    "        dl_va_p1 = DataLoader(ds_va_p1, batch_size=BATCH_VAL, shuffle=False)\n",
    "        dl_te_p1 = DataLoader(ds_te_p1, batch_size=BATCH_TEST, shuffle=False)\n",
    "\n",
    "        dl_tr_p2 = DataLoader(ds_tr_p2, batch_size=BATCH_TRAIN, shuffle=True)\n",
    "        dl_va_p2 = DataLoader(ds_va_p2, batch_size=BATCH_VAL, shuffle=False)\n",
    "        dl_te_p2 = DataLoader(ds_te_p2, batch_size=BATCH_TEST, shuffle=False)\n",
    "\n",
    "        # P3 reutiliza los mismos loaders que P2\n",
    "        dl_tr_p3, dl_va_p3, dl_te_p3 = dl_tr_p2, dl_va_p2, dl_te_p2\n",
    "\n",
    "        model_p1 = run_phase('P1', dl_tr_p1, dl_va_p1, in_channels=TS_CHANNELS)\n",
    "        model_p2 = run_phase('P2', dl_tr_p2, dl_va_p2, in_channels=TS_CHANNELS, p_dim=p_dim)\n",
    "        model_p3 = run_phase('P3', dl_tr_p3, dl_va_p3, in_channels=TS_CHANNELS, p_dim=p_dim)\n",
    "\n",
    "        # Test\n",
    "        _, te_auc_p1, te_pr_p1 = eval_one_epoch(model_p1, dl_te_p1, nn.BCEWithLogitsLoss())\n",
    "        _, te_auc_p2, te_pr_p2 = eval_one_epoch(model_p2, dl_te_p2, nn.BCEWithLogitsLoss())\n",
    "        _, te_auc_p3, te_pr_p3 = eval_one_epoch(model_p3, dl_te_p3, nn.BCEWithLogitsLoss())\n",
    "        te_results['P1'].append((te_auc_p1, te_pr_p1))\n",
    "        te_results['P2'].append((te_auc_p2, te_pr_p2))\n",
    "        te_results['P3'].append((te_auc_p3, te_pr_p3))\n",
    "        print(f\"Fold {fold} | Test AUROC P1/P2/P3 = {te_auc_p1:.3f}/{te_auc_p2:.3f}/{te_auc_p3:.3f} | \"\n",
    "              f\"AUPRC = {te_pr_p1:.3f}/{te_pr_p2:.3f}/{te_pr_p3:.3f}\")\n",
    "\n",
    "    # Resumen final\n",
    "    for p in ['P1','P2','P3']:\n",
    "        arr = np.array(te_results[p])\n",
    "        print(f\"{p} | Test mean AUROC={np.nanmean(arr[:,0]):.3f} | AUPRC={np.nanmean(arr[:,1]):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_mimic(Path(\"/home/gmartinez/Tesis/Datasets/MIMIC-III\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
