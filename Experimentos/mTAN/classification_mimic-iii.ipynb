{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ===== Configuración general =====\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Ventana fija por hora (48h)\n",
    "WINDOW_HOURS = 48\n",
    "L_MAX = WINDOW_HOURS  # pasos temporales\n",
    "TS_CHANNELS = 2       # HR + RR/MAP (o HR + Lactato)\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LR = 2e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 20\n",
    "GRAD_CLIP = 1.0\n",
    "BATCH_TRAIN = 64\n",
    "BATCH_VAL = 128\n",
    "BATCH_TEST = 128\n",
    "\n",
    "# ===== Carga/transformación de MIMIC =====\n",
    "def load_mimic_data(mimic_root: Path):\n",
    "    \"\"\"\n",
    "    Espera que tengas tablas/CSVs ya extraídos:\n",
    "    - chartevents_hourly.csv: columns [subject_id, hadm_id, icustay_id, chart_time_hour, HR, RR, MAP]\n",
    "    - labevents_hourly.csv: columns [subject_id, hadm_id, icustay_id, chart_time_hour, Lactate]\n",
    "    - admissions_labels.csv: [subject_id, hadm_id, icustay_id, admit_time, label_binary]\n",
    "    - profiles.csv: [subject_id, hadm_id, icustay_id, age, gender, admission_type, charlson, ...]\n",
    "    - notes_text.csv: [subject_id, hadm_id, icustay_id, note_time, note_text] (primera nota o conjunto)\n",
    "    \"\"\"\n",
    "    charts = pd.read_csv(mimic_root / 'chartevents_hourly.csv', parse_dates=['chart_time_hour'])\n",
    "    labs = pd.read_csv(mimic_root / 'labevents_hourly.csv', parse_dates=['chart_time_hour'])\n",
    "    labels = pd.read_csv(mimic_root / 'admissions_labels.csv', parse_dates=['admit_time'])\n",
    "    profiles = pd.read_csv(mimic_root / 'profiles.csv')\n",
    "    notes = pd.read_csv(mimic_root / 'notes_text.csv', parse_dates=['note_time'])\n",
    "\n",
    "    # Normaliza IDs a string\n",
    "    for df in (charts, labs, labels, profiles, notes):\n",
    "        for col in ['subject_id', 'hadm_id', 'icustay_id']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str)\n",
    "\n",
    "    return charts, labs, labels, profiles, notes\n",
    "\n",
    "def build_hourly_grid(start_time: pd.Timestamp, hours=L_MAX):\n",
    "    # Rejilla por hora desde start_time\n",
    "    tgrid = [start_time + pd.Timedelta(hours=h) for h in range(hours)]\n",
    "    return pd.DataFrame({'chart_time_hour': tgrid})\n",
    "\n",
    "def aggregate_signals(charts_user, labs_user, start_time, hours=L_MAX):\n",
    "    # Construye la rejilla y hace merge por hora\n",
    "    grid = build_hourly_grid(start_time.normalize(), hours=hours)  # puedes usar admit_time como base\n",
    "    # Merge charts (HR, RR/MAP)\n",
    "    charts_user = charts_user.copy()\n",
    "    charts_user['chart_time_hour'] = charts_user['chart_time_hour'].dt.floor('H')\n",
    "    merged = grid.merge(charts_user, on='chart_time_hour', how='left')\n",
    "\n",
    "    # Si usas labs (ej. Lactato) como segunda señal, mézclalo aquí\n",
    "    if labs_user is not None and not labs_user.empty:\n",
    "        labs_user = labs_user.copy()\n",
    "        labs_user['chart_time_hour'] = labs_user['chart_time_hour'].dt.floor('H')\n",
    "        merged = merged.merge(labs_user[['chart_time_hour','Lactate']], on='chart_time_hour', how='left')\n",
    "\n",
    "    # Selección de canales\n",
    "    # Opción A: HR + RR\n",
    "    v1 = merged['HR'].astype(float) if 'HR' in merged.columns else pd.Series([np.nan]*len(merged))\n",
    "    v2 = merged['RR'].astype(float) if 'RR' in merged.columns else (\n",
    "        merged['MAP'].astype(float) if 'MAP' in merged.columns else (\n",
    "            merged['Lactate'].astype(float) if 'Lactate' in merged.columns else pd.Series([np.nan]*len(merged))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    X = np.stack([\n",
    "        v1.fillna(np.nan).to_numpy(),\n",
    "        v2.fillna(np.nan).to_numpy()\n",
    "    ], axis=-1)  # shape (L, 2)\n",
    "\n",
    "    M = ~np.isnan(X)\n",
    "    X = np.where(M, X, 0.0)\n",
    "    # T normalizado 0..1\n",
    "    T = np.linspace(0.0, 1.0, len(merged), dtype=float)\n",
    "    M = M.astype(float)\n",
    "\n",
    "    return X.astype(float), T.astype(float), M.astype(float)\n",
    "\n",
    "def build_instances(charts, labs, labels, profiles, notes):\n",
    "    \"\"\"\n",
    "    Construye instancias por (subject_id, hadm_id, icustay_id).\n",
    "    Toma primeras 48h desde admit_time (o icu_in_time si lo tienes).\n",
    "    Genera X (L, C=2), T (L), M (L, C), y target binario; y perfil tabular P.\n",
    "    También produce embeddings de notas (opcional) para P.\n",
    "    \"\"\"\n",
    "    # Prepara embeddings simples de notas (TF-IDF + PCA → 64 dims)\n",
    "    # Nota: en producción usarías un encoder clínico; esto es para tener una baseline reproducible.\n",
    "    # Selecciona la primera nota por admisión\n",
    "    first_notes = notes.sort_values('note_time').groupby(['subject_id','hadm_id','icustay_id'], as_index=False).first()\n",
    "    text_corpus = first_notes['note_text'].fillna('')\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    X_tfidf = tfidf.fit_transform(text_corpus).astype(np.float32)\n",
    "    pca = PCA(n_components=64, random_state=SEED)\n",
    "    note_emb = pca.fit_transform(X_tfidf.toarray()) if X_tfidf.shape[0] > 0 else np.zeros((0,64), dtype=np.float32)\n",
    "    first_notes['note_emb'] = list(note_emb)\n",
    "\n",
    "    # Perfiles num/cat\n",
    "    prof = profiles.copy()\n",
    "    prof = prof.merge(first_notes[['subject_id','hadm_id','icustay_id','note_emb']], on=['subject_id','hadm_id','icustay_id'], how='left')\n",
    "    prof['note_emb'] = prof['note_emb'].apply(lambda x: np.array(x, dtype=np.float32) if isinstance(x, (list,np.ndarray)) else np.zeros((64,), dtype=np.float32))\n",
    "\n",
    "    # One-hot para categóricas, fillna mean para numéricas\n",
    "    cat_cols = [c for c in prof.columns if c in ('gender','admission_type')]\n",
    "    num_cols = [c for c in prof.columns if c not in cat_cols and c not in ('subject_id','hadm_id','icustay_id','note_emb')]\n",
    "    prof_num = prof[num_cols].apply(lambda col: col.fillna(col.mean()))\n",
    "    prof_cat = pd.get_dummies(prof[cat_cols], prefix=cat_cols) if len(cat_cols)>0 else pd.DataFrame(index=prof.index)\n",
    "    P_tab = pd.concat([prof_num, prof_cat], axis=1)\n",
    "\n",
    "    # Concatena note_emb a P_tab\n",
    "    P_all = []\n",
    "    for i in range(len(prof)):\n",
    "        vec_tab = P_tab.iloc[i].to_numpy(dtype=np.float32)\n",
    "        vec_note = prof.iloc[i]['note_emb']\n",
    "        P_all.append(np.concatenate([vec_tab, vec_note], axis=0))\n",
    "    # Estandariza P_all canal a canal\n",
    "    P_mat = np.stack(P_all, axis=0)\n",
    "    P_mat = (P_mat - np.nanmean(P_mat, axis=0)) / (np.nanstd(P_mat, axis=0) + 1e-6)\n",
    "\n",
    "    # Empareja con labels\n",
    "    lab = labels[['subject_id','hadm_id','icustay_id','admit_time','label_binary']].copy()\n",
    "    lab['label_binary'] = lab['label_binary'].astype(int)\n",
    "    merged = lab.merge(prof[['subject_id','hadm_id','icustay_id']], on=['subject_id','hadm_id','icustay_id'], how='inner')\n",
    "    # Construye X/T/M por instancia\n",
    "    X_list, T_list, M_list, y_list, meta, P_list = [], [], [], [], [], []\n",
    "\n",
    "    # Índices para reindexar P_mat\n",
    "    key_to_index = {tuple(row[['subject_id','hadm_id','icustay_id']]): idx for idx, row in prof.reset_index(drop=True).iterrows()}\n",
    "\n",
    "    for _, row in merged.iterrows():\n",
    "        key = (row['subject_id'], row['hadm_id'], row['icustay_id'])\n",
    "        admit_time = pd.to_datetime(row['admit_time'])\n",
    "        # Filtra datos del usuario/admisión\n",
    "        charts_user = charts[(charts['subject_id']==key[0]) & (charts['hadm_id']==key[1]) & (charts['icustay_id']==key[2])]\n",
    "        labs_user = labs[(labs['subject_id']==key[0]) & (labs['hadm_id']==key[1]) & (labs['icustay_id']==key[2])] if labs is not None else None\n",
    "        X, T, M = aggregate_signals(charts_user, labs_user, admit_time, hours=L_MAX)\n",
    "        y = int(row['label_binary'])\n",
    "        X_list.append(X); T_list.append(T); M_list.append(M); y_list.append(y); meta.append(key)\n",
    "        P_list.append(P_mat[key_to_index[key]])\n",
    "\n",
    "    return X_list, T_list, M_list, np.array(y_list, dtype=int), meta, np.stack(P_list, axis=0)\n",
    "\n",
    "# ===== Datasets =====\n",
    "class TimeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, T, M, y, user_ids=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.T = torch.tensor(T, dtype=torch.float32)\n",
    "        self.M = torch.tensor(M, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.user_ids = user_ids if user_ids is not None else [None]*len(y)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.T[idx], self.M[idx], self.y[idx], self.user_ids[idx]\n",
    "\n",
    "class TimeDatasetWithProfile(TimeDataset):\n",
    "    def __init__(self, X, T, M, y, P, user_ids=None):\n",
    "        super().__init__(X, T, M, y, user_ids)\n",
    "        self.P = torch.tensor(P, dtype=torch.float32)\n",
    "    def __getitem__(self, idx):\n",
    "        X, T, M, y, uid = super().__getitem__(idx)\n",
    "        return X, T, M, y, uid, self.P[idx]\n",
    "\n",
    "# ===== Modelo (reuso de tus bloques) =====\n",
    "# Copia aquí TimeAttentionBlock, MTANBackbone, FiLMGenerator, HeadMLP, ModelPhase1_TS, ModelPhase2_TSProfile, ModelPhase3_FiLM\n",
    "# (idénticos a tu código original, salvo que in_channels=TS_CHANNELS)\n",
    "\n",
    "# ===== Utilidades =====\n",
    "def compute_channel_stats(X_train, M_train):\n",
    "    B, L, C = X_train.shape\n",
    "    means = np.zeros((C,), dtype=float)\n",
    "    stds = np.ones((C,), dtype=float)\n",
    "    for c in range(C):\n",
    "        vals = X_train[..., c][M_train[..., c] == 1.0]\n",
    "        means[c] = float(np.mean(vals)) if vals.size>0 else 0.0\n",
    "        stds[c] = float(np.std(vals) + 1e-6) if vals.size>0 else 1.0\n",
    "    return means, stds\n",
    "\n",
    "def standardize_by_stats(X, M, means, stds):\n",
    "    X_std = (X - means[None, None, :]) / stds[None, None, :]\n",
    "    return np.where(M == 1.0, X_std, 0.0)\n",
    "\n",
    "# Entrenamiento/Evaluación (igual que tu código)\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    logits_all, targets_all = [], []\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        if len(batch) == 5:\n",
    "            X, T, M, y, _ = batch\n",
    "            X, T, M, y = X.to(DEVICE), T.to(DEVICE), M.to(DEVICE), y.to(DEVICE)\n",
    "            logits = model(X, T, M)\n",
    "        else:\n",
    "            X, T, M, y, _, P = batch\n",
    "            X, T, M, y, P = X.to(DEVICE), T.to(DEVICE), M.to(DEVICE), y.to(DEVICE), P.to(DEVICE)\n",
    "            logits = model(X, T, M, P)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        logits_all.append(logits.detach().cpu().numpy())\n",
    "        targets_all.append(y.detach().cpu().numpy())\n",
    "    yhat = np.concatenate(logits_all); yt = np.concatenate(targets_all)\n",
    "    try: auroc = roc_auc_score(yt, yhat)\n",
    "    except: auroc = np.nan\n",
    "    try: auprc = average_precision_score(yt, yhat)\n",
    "    except: auprc = np.nan\n",
    "    return total_loss / len(yt), auroc, auprc\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    logits_all, targets_all = [], []\n",
    "    for batch in loader:\n",
    "        if len(batch) == 5:\n",
    "            X, T, M, y, _ = batch\n",
    "            X, T, M, y = X.to(DEVICE), T.to(DEVICE), M.to(DEVICE), y.to(DEVICE)\n",
    "            logits = model(X, T, M)\n",
    "        else:\n",
    "            X, T, M, y, _, P = batch\n",
    "            X, T, M, y, P = X.to(DEVICE), T.to(DEVICE), M.to(DEVICE), y.to(DEVICE), P.to(DEVICE)\n",
    "            logits = model(X, T, M, P)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        logits_all.append(logits.detach().cpu().numpy())\n",
    "        targets_all.append(y.detach().cpu().numpy())\n",
    "    yhat = np.concatenate(logits_all); yt = np.concatenate(targets_all)\n",
    "    try: auroc = roc_auc_score(yt, yhat)\n",
    "    except: auroc = np.nan\n",
    "    try: auprc = average_precision_score(yt, yhat)\n",
    "    except: auprc = np.nan\n",
    "    return total_loss / len(yt), auroc, auprc\n",
    "\n",
    "def run_phase(phase_name, train_loader, val_loader, in_channels, p_dim=None):\n",
    "    if phase_name == 'P1':\n",
    "        model = ModelPhase1_TS(in_channels).to(DEVICE)\n",
    "    elif phase_name == 'P2':\n",
    "        assert p_dim is not None\n",
    "        model = ModelPhase2_TSProfile(in_channels, p_dim).to(DEVICE)\n",
    "    elif phase_name == 'P3':\n",
    "        assert p_dim is not None\n",
    "        model = ModelPhase3_FiLM(in_channels, p_dim).to(DEVICE)\n",
    "    else:\n",
    "        raise ValueError(\"phase_name must be P1, P2 or P3\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val = -np.inf\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        tl, tr_auc, tr_pr = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        vl, va_auc, va_pr = eval_one_epoch(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "        score = np.nanmean([va_auc, va_pr])\n",
    "        if score > best_val:\n",
    "            best_val = score\n",
    "            best_state = { 'model': model.state_dict() }\n",
    "        print(f\"{phase_name} | Epoch {ep:02d} | train_loss={tl:.4f} val_loss={vl:.4f} | \"\n",
    "              f\"AUROC train/val={tr_auc:.3f}/{va_auc:.3f} | AUPRC train/val={tr_pr:.3f}/{va_pr:.3f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state['model'])\n",
    "    return model\n",
    "\n",
    "# ===== Pipeline principal =====\n",
    "def main_mimic(mimic_root: Path):\n",
    "    charts, labs, labels, profiles, notes = load_mimic_data(mimic_root)\n",
    "\n",
    "    X_list, T_list, M_list, y, meta, P_vals = build_instances(charts, labs, labels, profiles, notes)\n",
    "    # Reindex y preparar arrays\n",
    "    X_all = np.stack(X_list, axis=0)  # (N, L, C=2)\n",
    "    T_all = np.stack(T_list, axis=0)  # (N, L)\n",
    "    M_all = np.stack(M_list, axis=0)  # (N, L, C)\n",
    "    user_ids = [f\"{s}-{h}-{i}\" for (s,h,i) in meta]\n",
    "    in_channels = X_all.shape[-1]\n",
    "    p_dim = P_vals.shape[1]\n",
    "\n",
    "    # Holdout test por HADM_ID (o SUBJECT_ID si prefieres)\n",
    "    groups = np.array([m[1] for m in meta])  # hadm_id\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "    trainval_idx, test_idx = next(gss.split(X_all, y, groups=groups))\n",
    "    X_tv, T_tv, M_tv, y_tv = X_all[trainval_idx], T_all[trainval_idx], M_all[trainval_idx], y[trainval_idx]\n",
    "    X_te, T_te, M_te, y_te = X_all[test_idx], T_all[test_idx], M_all[test_idx], y[test_idx]\n",
    "    P_tv, P_te = P_vals[trainval_idx], P_vals[test_idx]\n",
    "    groups_tv = groups[trainval_idx]\n",
    "\n",
    "    # Validación interna por grupos\n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    te_results = {'P1': [], 'P2': [], 'P3': []}\n",
    "\n",
    "    for fold, (tr_idx_rel, va_idx_rel) in enumerate(gkf.split(X_tv, y_tv, groups=groups_tv), start=1):\n",
    "        print(f\"=== Fold {fold} ===\")\n",
    "        X_tr, T_tr, M_tr, y_tr = X_tv[tr_idx_rel], T_tv[tr_idx_rel], M_tv[tr_idx_rel], y_tv[tr_idx_rel]\n",
    "        X_va, T_va, M_va, y_va = X_tv[va_idx_rel], T_tv[va_idx_rel], M_tv[va_idx_rel], y_tv[va_idx_rel]\n",
    "        P_tr, P_va = P_tv[tr_idx_rel], P_tv[va_idx_rel]\n",
    "\n",
    "        means, stds = compute_channel_stats(X_tr, M_tr)\n",
    "        X_tr_std = standardize_by_stats(X_tr, M_tr, means, stds)\n",
    "        X_va_std = standardize_by_stats(X_va, M_va, means, stds)\n",
    "        X_te_std = standardize_by_stats(X_te, M_te, means, stds)\n",
    "\n",
    "        ds_tr_p1 = TimeDataset(X_tr_std, T_tr, M_tr, y_tr)\n",
    "        ds_va_p1 = TimeDataset(X_va_std, T_va, M_va, y_va)\n",
    "        ds_te_p1 = TimeDataset(X_te_std, T_te, M_te, y_te)\n",
    "\n",
    "        ds_tr_p2 = TimeDatasetWithProfile(X_tr_std, T_tr, M_tr, y_tr, P_tr)\n",
    "        ds_va_p2 = TimeDatasetWithProfile(X_va_std, T_va, M_va, y_va, P_va)\n",
    "        ds_te_p2 = TimeDatasetWithProfile(X_te_std, T_te, M_te, y_te, P_te)\n",
    "\n",
    "        dl_tr_p1 = DataLoader(ds_tr_p1, batch_size=BATCH_TRAIN, shuffle=True)\n",
    "        dl_va_p1 = DataLoader(ds_va_p1, batch_size=BATCH_VAL, shuffle=False)\n",
    "        dl_te_p1 = DataLoader(ds_te_p1, batch_size=BATCH_TEST, shuffle=False)\n",
    "\n",
    "        dl_tr_p2 = DataLoader(ds_tr_p2, batch_size=BATCH_TRAIN, shuffle=True)\n",
    "        dl_va_p2 = DataLoader(ds_va_p2, batch_size=BATCH_VAL, shuffle=False)\n",
    "        dl_te_p2 = DataLoader(ds_te_p2, batch_size=BATCH_TEST, shuffle=False)\n",
    "\n",
    "        # P3 reutiliza los mismos loaders que P2\n",
    "        dl_tr_p3, dl_va_p3, dl_te_p3 = dl_tr_p2, dl_va_p2, dl_te_p2\n",
    "\n",
    "        model_p1 = run_phase('P1', dl_tr_p1, dl_va_p1, in_channels)\n",
    "        model_p2 = run_phase('P2', dl_tr_p2, dl_va_p2, in_channels, p_dim=p_dim)\n",
    "        model_p3 = run_phase('P3', dl_tr_p3, dl_va_p3, in_channels, p_dim=p_dim)\n",
    "\n",
    "        # Test\n",
    "        _, te_auc_p1, te_pr_p1 = eval_one_epoch(model_p1, dl_te_p1, nn.BCEWithLogitsLoss())\n",
    "        _, te_auc_p2, te_pr_p2 = eval_one_epoch(model_p2, dl_te_p2, nn.BCEWithLogitsLoss())\n",
    "        _, te_auc_p3, te_pr_p3 = eval_one_epoch(model_p3, dl_te_p3, nn.BCEWithLogitsLoss())\n",
    "        te_results['P1'].append((te_auc_p1, te_pr_p1))\n",
    "        te_results['P2'].append((te_auc_p2, te_pr_p2))\n",
    "        te_results['P3'].append((te_auc_p3, te_pr_p3))\n",
    "        print(f\"Fold {fold} | Test AUROC P1/P2/P3 = {te_auc_p1:.3f}/{te_auc_p2:.3f}/{te_auc_p3:.3f} | \"\n",
    "              f\"AUPRC = {te_pr_p1:.3f}/{te_pr_p2:.3f}/{te_pr_p3:.3f}\")\n",
    "\n",
    "    # Resumen final\n",
    "    for p in ['P1','P2','P3']:\n",
    "        arr = np.array(te_results[p])\n",
    "        print(f\"{p} | Test mean AUROC={np.nanmean(arr[:,0]):.3f} | AUPRC={np.nanmean(arr[:,1]):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_mimic(Path(\"/path/to/mimic_preprocessed\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
