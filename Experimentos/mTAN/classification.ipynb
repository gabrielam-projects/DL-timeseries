{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('/home/gmartinez/Tesis/Datasets/Synthetic-data/outputs')  # cambia según tu entorno\n",
    "HR_CSV = DATA_DIR / \"hr_series.csv\"\n",
    "SLEEP_CSV = DATA_DIR / \"sleep_series.csv\"\n",
    "LABELS_CSV = DATA_DIR / \"nightly_labeled.csv\"\n",
    "PROFILES_CSV = DATA_DIR / \"user_profiles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar\n",
    "hr_data = pd.read_csv(HR_CSV, parse_dates=['timestamp'])\n",
    "sleep_data = pd.read_csv(SLEEP_CSV, parse_dates=['night_date'])\n",
    "labels = pd.read_csv(LABELS_CSV, parse_dates=['night_date'])\n",
    "\n",
    "# Normalizar tipos\n",
    "hr_data['user_id'] = hr_data['user_id'].astype(str)\n",
    "sleep_data['user_id'] = sleep_data['user_id'].astype(str)\n",
    "labels['user_id'] = labels['user_id'].astype(str)\n",
    "\n",
    "# Derivar fecha (día) para HR y contar eventos diarios (tu serie ejemplo es un conteo por día)\n",
    "hr_data['date'] = hr_data['timestamp'].dt.floor('D')\n",
    "\n",
    "# Serie diaria de HR (puedes reemplazar \"count\" por otra métrica; p.ej. mediana HR)\n",
    "hr_daily = (\n",
    "    hr_data\n",
    "    .groupby(['user_id','date'], as_index=False)\n",
    "    .agg(hr_count=('hr','count'),\n",
    "         hr_med=('hr','median'))  # opcional, por si luego prefieres mediana\n",
    ")\n",
    "\n",
    "# Para el baseline usaremos hr_count como la serie diaria principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>night_date</th>\n",
       "      <th>total_hours</th>\n",
       "      <th>deep_minutes</th>\n",
       "      <th>rem_minutes</th>\n",
       "      <th>awake_minutes</th>\n",
       "      <th>sleep_latency_minutes</th>\n",
       "      <th>awakenings</th>\n",
       "      <th>rhr_median</th>\n",
       "      <th>hrv_proxy_median</th>\n",
       "      <th>...</th>\n",
       "      <th>score_S_rem</th>\n",
       "      <th>score_S_awake</th>\n",
       "      <th>score_S_latency</th>\n",
       "      <th>score_C_rhr</th>\n",
       "      <th>score_C_hrv</th>\n",
       "      <th>score_C_resp</th>\n",
       "      <th>recovery_score_0_1</th>\n",
       "      <th>recovery_threshold</th>\n",
       "      <th>recovery_label_binary</th>\n",
       "      <th>failure_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>5.766667</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.310667</td>\n",
       "      <td>0.349333</td>\n",
       "      <td>Deficient Recovery</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>140</td>\n",
       "      <td>10</td>\n",
       "      <td>118</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.330633</td>\n",
       "      <td>0.349333</td>\n",
       "      <td>Deficient Recovery</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>6.233333</td>\n",
       "      <td>146</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.329333</td>\n",
       "      <td>0.349333</td>\n",
       "      <td>Deficient Recovery</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>7.266667</td>\n",
       "      <td>176</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370667</td>\n",
       "      <td>0.349333</td>\n",
       "      <td>Adequate Recovery</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>7.416667</td>\n",
       "      <td>134</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.349333</td>\n",
       "      <td>Adequate Recovery</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id night_date  total_hours  deep_minutes  rem_minutes  awake_minutes  \\\n",
       "0       1 2024-01-01     5.766667           128            8             95   \n",
       "1       1 2024-01-02     6.333333           140           10            118   \n",
       "2       1 2024-01-03     6.233333           146           16             77   \n",
       "3       1 2024-01-04     7.266667           176           16             82   \n",
       "4       1 2024-01-05     7.416667           134           12            128   \n",
       "\n",
       "   sleep_latency_minutes  awakenings  rhr_median  hrv_proxy_median  ...  \\\n",
       "0                      2          42         NaN               NaN  ...   \n",
       "1                      6          32         NaN               NaN  ...   \n",
       "2                      1          34         NaN               NaN  ...   \n",
       "3                      1          34         NaN               NaN  ...   \n",
       "4                      2          45         NaN               NaN  ...   \n",
       "\n",
       "   score_S_rem  score_S_awake  score_S_latency  score_C_rhr  score_C_hrv  \\\n",
       "0          0.0            0.0            1.000          NaN          NaN   \n",
       "1          0.0            0.0            0.975          NaN          NaN   \n",
       "2          0.0            0.0            1.000          NaN          NaN   \n",
       "3          0.0            0.0            1.000          NaN          NaN   \n",
       "4          0.0            0.0            1.000          NaN          NaN   \n",
       "\n",
       "   score_C_resp  recovery_score_0_1  recovery_threshold  \\\n",
       "0           NaN            0.310667            0.349333   \n",
       "1           NaN            0.330633            0.349333   \n",
       "2           NaN            0.329333            0.349333   \n",
       "3           NaN            0.370667            0.349333   \n",
       "4           NaN            0.376667            0.349333   \n",
       "\n",
       "   recovery_label_binary  failure_minutes  \n",
       "0     Deficient Recovery               63  \n",
       "1     Deficient Recovery               29  \n",
       "2     Deficient Recovery               46  \n",
       "3      Adequate Recovery               56  \n",
       "4      Adequate Recovery               49  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear etiqueta binaria a 0/1\n",
    "label_map = {\n",
    "    'Adequate Recovery': 1,\n",
    "    'Deficient Recovery': 0\n",
    "}\n",
    "labels['target'] = labels['recovery_label_binary'].map(label_map).astype(int)  # ajusta el nombre de la columna real\n",
    "\n",
    "# Índice maestro de noches etiquetadas\n",
    "nights = labels[['user_id','night_date','target']].dropna().copy()\n",
    "nights['night_date'] = pd.to_datetime(nights['night_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def extract_hr_window(hr_daily_user, night_date, past_days=7, include_night=True, feature='hr_count'):\n",
    "    \"\"\"\n",
    "    Devuelve vector de longitud L = past_days + (1 si include_night) con HR diaria.\n",
    "    Rellena con 0 si faltan días (o usa np.nan y luego imputación).\n",
    "    \"\"\"\n",
    "    L = past_days + (1 if include_night else 0)\n",
    "    start_date = (night_date - timedelta(days=past_days)) if include_night else (night_date - timedelta(days=past_days))\n",
    "    end_date = night_date if include_night else (night_date - timedelta(days=1))\n",
    "    idx_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "    # Merge con serie del usuario\n",
    "    series = (\n",
    "        pd.DataFrame({'date': idx_days})\n",
    "        .merge(hr_daily_user[['date', feature]], on='date', how='left')\n",
    "        [feature]\n",
    "        .fillna(0.0)  # baseline: 0 al faltar datos; alternativa: forward-fill/back-fill/median\n",
    "        .to_numpy(dtype=float)\n",
    "    )\n",
    "    if len(series) != L:\n",
    "        # Por seguridad\n",
    "        series = np.resize(series, L)\n",
    "    return series\n",
    "\n",
    "SLEEP_FEATURES = ['total_hours','deep_minutes','rem_minutes','awake_minutes',\n",
    "                  'sleep_latency_minutes','awakenings','failure_minutes']\n",
    "\n",
    "def extract_sleep_features_for_night(sleep_user, night_date, features=SLEEP_FEATURES):\n",
    "    row = sleep_user.loc[sleep_user['night_date'] == night_date, features]\n",
    "    if row.empty:\n",
    "        return np.array([np.nan]*len(features), dtype=float)\n",
    "    return row.iloc[0].astype(float).to_numpy()\n",
    "\n",
    "def standardize_fit(X, eps=1e-8):\n",
    "    mean = np.nanmean(X, axis=0)\n",
    "    std = np.nanstd(X, axis=0)\n",
    "    std = np.where(std < eps, 1.0, std)\n",
    "    return mean, std\n",
    "\n",
    "def standardize_transform(X, mean, std):\n",
    "    return (X - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_samples(hr_daily, sleep_data, nights, past_days=7, use_hr_feature='hr_count'):\n",
    "    # Pre-index por usuario para eficiencia\n",
    "    hr_by_user = {u: df.sort_values('date') for u, df in hr_daily.groupby('user_id')}\n",
    "    sleep_by_user = {u: df.sort_values('night_date') for u, df in sleep_data.groupby('user_id')}\n",
    "\n",
    "    X_list = []      # features tensor (variable length → luego padding)\n",
    "    T_list = []      # timestamps por canal\n",
    "    M_list = []      # masks por canal\n",
    "    y_list = []\n",
    "    meta_list = []   # (user_id, night_date)\n",
    "\n",
    "    for _, row in nights.iterrows():\n",
    "        u = row['user_id']\n",
    "        night = row['night_date']\n",
    "        target = row['target']\n",
    "\n",
    "        hr_user = hr_by_user.get(u, pd.DataFrame(columns=['date',use_hr_feature]))\n",
    "        sleep_user = sleep_by_user.get(u, pd.DataFrame(columns=['night_date'] + SLEEP_FEATURES))\n",
    "\n",
    "        # Canal HR\n",
    "        hr_vec = extract_hr_window(hr_user, night_date=night, past_days=past_days,\n",
    "                                   include_night=True, feature=use_hr_feature)  # (L1,)\n",
    "        L1 = len(hr_vec)\n",
    "        # timestamps uniformes en [0,1]\n",
    "        t_hr = np.linspace(0.0, 1.0, L1)\n",
    "        m_hr = (~np.isnan(hr_vec)).astype(float)\n",
    "\n",
    "        # Canal Sleep (longitud 1, multi-features)\n",
    "        s_vec = extract_sleep_features_for_night(sleep_user, night, features=SLEEP_FEATURES)  # (C2,)\n",
    "        L2 = 1\n",
    "        t_sl = np.array([1.0])  # situamos la noche objetivo al final de la ventana\n",
    "        m_sl = (~np.isnan(s_vec)).astype(float)  # para C2 canales aplicaremos máscara por canal\n",
    "\n",
    "        # Montaje multicanal:\n",
    "        # Representamos como dos canales separados, cada uno con su propio conjunto de features.\n",
    "        # Para mTAN, típica entrada: (B, L, C) con timestamps y máscara (B, L, C).\n",
    "        # Aquí unificamos longitudes concatenando en el eje temporal y rellenamos con ceros/máscara 0.\n",
    "        # Canal HR: C_hr = 1, Canal Sleep: C_sl = len(SLEEP_FEATURES). Concatenaremos features en eje C, alineando tiempo con padding.\n",
    "\n",
    "        # Construir secuencia temporal concatenando puntos HR y Sleep\n",
    "        t_seq = np.concatenate([t_hr, t_sl], axis=0)                 # (L1+1,)\n",
    "        # Features: creamos C = 1 + len(SLEEP_FEATURES)\n",
    "        C = 1 + len(SLEEP_FEATURES)\n",
    "        X = np.zeros((L1+1, C), dtype=float)\n",
    "        M = np.zeros((L1+1, C), dtype=float)\n",
    "\n",
    "        # Rellenar HR en canal 0 para los L1 primeros pasos\n",
    "        X[:L1, 0] = np.nan_to_num(hr_vec, nan=0.0)\n",
    "        M[:L1, 0] = m_hr\n",
    "\n",
    "        # Rellenar Sleep en el último paso temporal, canales 1..C-1\n",
    "        X[L1, 1:] = np.nan_to_num(s_vec, nan=0.0)\n",
    "        M[L1, 1:] = (~np.isnan(s_vec)).astype(float)\n",
    "\n",
    "        X_list.append(X)\n",
    "        T_list.append(t_seq)\n",
    "        M_list.append(M)\n",
    "        y_list.append(target)\n",
    "        meta_list.append((u, night))\n",
    "\n",
    "    return X_list, T_list, M_list, np.array(y_list, dtype=int), meta_list\n",
    "\n",
    "# Construcción\n",
    "X_list, T_list, M_list, y, meta = build_samples(hr_daily, sleep_data, nights, past_days=7, use_hr_feature='hr_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Split por usuario para evitar fuga\n",
    "users = np.array([u for u, _ in meta])\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "(train_idx, test_idx) = next(gss.split(np.zeros(len(users)), groups=users))\n",
    "\n",
    "def stack_fixed_length(X_list, T_list, M_list):\n",
    "    # Si todas las secuencias tienen la misma longitud, basta apilar.\n",
    "    L = X_list[0].shape[0]\n",
    "    C = X_list[0].shape[1]\n",
    "    B = len(X_list)\n",
    "    X = np.stack(X_list, axis=0)    # (B, L, C)\n",
    "    T = np.stack([t for t in T_list], axis=0)  # (B, L)\n",
    "    M = np.stack(M_list, axis=0)    # (B, L, C)\n",
    "    return X, T, M\n",
    "\n",
    "X_all, T_all, M_all = stack_fixed_length(X_list, T_list, M_list)\n",
    "\n",
    "# Estimación de mean/std solo con train\n",
    "X_train = X_all[train_idx]\n",
    "M_train = M_all[train_idx]\n",
    "\n",
    "# Calcular stats por canal usando solo elementos observados (M=1)\n",
    "obs_mask = M_train.astype(bool)\n",
    "sum_x = (X_train * obs_mask).sum(axis=(0,1))\n",
    "count_x = obs_mask.sum(axis=(0,1)).clip(min=1)\n",
    "mean_x = sum_x / count_x\n",
    "\n",
    "sum_sq = ((X_train - mean_x) * obs_mask)**2\n",
    "var_x = sum_sq.sum(axis=(0,1)) / count_x\n",
    "std_x = np.sqrt(np.maximum(var_x, 1e-8))\n",
    "\n",
    "def apply_standardize(X, M, mean_x, std_x):\n",
    "    return (X - mean_x) / std_x\n",
    "\n",
    "X_std = apply_standardize(X_all, M_all, mean_x, std_x)\n",
    "\n",
    "X_train, T_train, M_train, y_train = X_std[train_idx], T_all[train_idx], M_all[train_idx], y[train_idx]\n",
    "X_test,  T_test,  M_test,  y_test  = X_std[test_idx],  T_all[test_idx],  M_all[test_idx],  y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TimeAwareAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_model, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dk = d_model // n_heads\n",
    "        assert d_model % n_heads == 0\n",
    "        self.Wq = nn.Linear(d_in, d_model)\n",
    "        self.Wk = nn.Linear(d_in, d_model)\n",
    "        self.Wv = nn.Linear(d_in, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.time_decay = nn.Parameter(torch.tensor(1.0))  # factor de decaimiento temporal\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, T, M):\n",
    "        \"\"\"\n",
    "        X: (B, L, C_in) ya estandarizado\n",
    "        T: (B, L) en [0,1]\n",
    "        M: (B, L, C_in) máscara de observación; aquí usamos una máscara por tiempo: si al menos 1 canal observado\n",
    "        \"\"\"\n",
    "        B, L, Cin = X.shape\n",
    "        # Reducimos canales a una representación por tiempo mediante proyección\n",
    "        # Alternativa: sumar por canales con pesos aprendibles\n",
    "        X_in = X  # (B, L, C)\n",
    "        # Proyección a espacio de atención\n",
    "        Q = self.Wq(X_in)  # (B, L, d_model)\n",
    "        K = self.Wk(X_in)\n",
    "        V = self.Wv(X_in)\n",
    "\n",
    "        # Multi-head split\n",
    "        def split_heads(Z):\n",
    "            return Z.view(B, L, self.n_heads, self.dk).transpose(1,2)  # (B, h, L, dk)\n",
    "        Qh, Kh, Vh = split_heads(Q), split_heads(K), split_heads(V)\n",
    "\n",
    "        # Similitud + sesgo temporal\n",
    "        # Distancia temporal |ti - tj|\n",
    "        # Usamos T medio por paso; si hubiera múltiples canales con distintos tiempos, promediaríamos.\n",
    "        t = T.unsqueeze(1).unsqueeze(-1)  # (B,1,L,1)\n",
    "        # scores: (B, h, L, L)\n",
    "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / np.sqrt(self.dk)\n",
    "\n",
    "        # Penalización por distancia temporal\n",
    "        # dist[i,j] = |ti - tj|\n",
    "        Ti = T.unsqueeze(1).unsqueeze(-1)  # (B,1,L,1)\n",
    "        Tj = T.unsqueeze(1).unsqueeze(-2)  # (B,1,1,L)\n",
    "        dist = torch.abs(Ti - Tj)          # (B,1,L,L)\n",
    "        scores = scores - self.time_decay.abs() * dist  # mayor distancia → menor atención\n",
    "\n",
    "        # Máscara temporal: pasos sin observación en K\n",
    "        time_mask = (M.sum(dim=-1) > 0).unsqueeze(1).unsqueeze(2)  # (B,1,1,L_K)\n",
    "        scores = scores.masked_fill(~time_mask, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        Z = torch.matmul(attn, Vh)  # (B, h, L, dk)\n",
    "        Z = Z.transpose(1,2).contiguous().view(B, L, self.d_model)\n",
    "        return self.out(Z), attn\n",
    "\n",
    "class MTANClassifier(nn.Module):\n",
    "    def __init__(self, c_in, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(c_in, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                TimeAwareAttention(d_model, d_model, n_heads=n_heads, dropout=dropout),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(d_model, d_model*2),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(d_model*2, d_model)\n",
    "                ),\n",
    "                nn.LayerNorm(d_model),\n",
    "            ]) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, T, M):\n",
    "        # X: (B,L,C), T:(B,L), M:(B,L,C)\n",
    "        B,L,C = X.shape\n",
    "        x = self.input_proj(X)  # (B,L,d_model)\n",
    "        for attn, ln1, ff, ln2 in self.layers:\n",
    "            h, _ = attn(x, T, M)\n",
    "            x = ln1(x + h)\n",
    "            f = ff(x)\n",
    "            x = ln2(x + f)\n",
    "        # Pooling temporal con máscara\n",
    "        time_mask = (M.sum(dim=-1) > 0).float()  # (B,L)\n",
    "        masked_x = x * time_mask.unsqueeze(-1)\n",
    "        denom = time_mask.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "        pooled = masked_x.sum(dim=1) / denom  # (B,d_model)\n",
    "        logit = self.cls(pooled).squeeze(-1)   # (B,)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeDataset(Dataset):\n",
    "    def __init__(self, X, T, M, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.T = torch.tensor(T, dtype=torch.float32)\n",
    "        self.M = torch.tensor(M, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.T[idx], self.M[idx], self.y[idx]\n",
    "\n",
    "train_ds = TimeDataset(X_train, T_train, M_train, y_train)\n",
    "test_ds  = TimeDataset(X_test,  T_test,  M_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, drop_last=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Test AUROC 0.9970 | AUPRC 0.9952\n",
      "Epoch 02 | Test AUROC 0.9987 | AUPRC 0.9984\n",
      "Epoch 03 | Test AUROC 0.9993 | AUPRC 0.9987\n",
      "Epoch 04 | Test AUROC 0.9946 | AUPRC 0.9958\n",
      "Epoch 05 | Test AUROC 0.9972 | AUPRC 0.9977\n",
      "Epoch 06 | Test AUROC 0.9875 | AUPRC 0.9901\n",
      "Epoch 07 | Test AUROC 0.9981 | AUPRC 0.9979\n",
      "Epoch 08 | Test AUROC 0.9991 | AUPRC 0.9985\n",
      "Epoch 09 | Test AUROC 0.9996 | AUPRC 0.9994\n",
      "Epoch 10 | Test AUROC 0.9995 | AUPRC 0.9993\n",
      "Epoch 11 | Test AUROC 0.9997 | AUPRC 0.9995\n",
      "Epoch 12 | Test AUROC 0.9997 | AUPRC 0.9996\n",
      "Epoch 13 | Test AUROC 0.9998 | AUPRC 0.9997\n",
      "Epoch 14 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "Epoch 15 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "Epoch 16 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "Epoch 17 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "Epoch 18 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "Epoch 19 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "Epoch 20 | Test AUROC 0.9999 | AUPRC 0.9999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MTANClassifier(c_in=X_train.shape[2], d_model=128, n_heads=4, n_layers=2, dropout=0.2).to(device)\n",
    "\n",
    "# Pérdida BCE con logits; alternativa: focal si hay fuerte desbalance\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, Tb, Mb, yb in loader:\n",
    "            Xb, Tb, Mb = Xb.to(device), Tb.to(device), Mb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logit = model(Xb, Tb, Mb)\n",
    "            prob = torch.sigmoid(logit)\n",
    "            ys.append(yb.detach().cpu().numpy())\n",
    "            ps.append(prob.detach().cpu().numpy())\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_prob = np.concatenate(ps)\n",
    "    auroc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
    "    auprc = average_precision_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
    "    return auroc, auprc\n",
    "\n",
    "best = {'auroc': -1, 'state': None}\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for Xb, Tb, Mb, yb in train_loader:\n",
    "        Xb, Tb, Mb = Xb.to(device), Tb.to(device), Mb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logit = model(Xb, Tb, Mb)\n",
    "        loss = criterion(logit, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    auroc, auprc = evaluate(model, test_loader)\n",
    "    if auroc > best['auroc']:\n",
    "        best['auroc'] = auroc\n",
    "        best['state'] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    print(f\"Epoch {epoch:02d} | Test AUROC {auroc:.4f} | AUPRC {auprc:.4f}\")\n",
    "\n",
    "# Restaurar mejor estado (opcional)\n",
    "if best['state'] is not None:\n",
    "    model.load_state_dict(best['state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Profile information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase 1] Epoch 01 | Test AUROC 0.9984 | AUPRC 0.9976\n",
      "[Phase 1] Epoch 02 | Test AUROC 0.9995 | AUPRC 0.9992\n",
      "[Phase 1] Epoch 03 | Test AUROC 0.9997 | AUPRC 0.9995\n",
      "[Phase 1] Epoch 04 | Test AUROC 0.9997 | AUPRC 0.9994\n",
      "[Phase 1] Epoch 05 | Test AUROC 0.9968 | AUPRC 0.9952\n",
      "[Phase 1] Epoch 06 | Test AUROC 0.9995 | AUPRC 0.9992\n",
      "[Phase 1] Epoch 07 | Test AUROC 0.9998 | AUPRC 0.9997\n",
      "[Phase 1] Epoch 08 | Test AUROC 0.9998 | AUPRC 0.9997\n",
      "[Phase 1] Epoch 09 | Test AUROC 0.9996 | AUPRC 0.9993\n",
      "[Phase 1] Epoch 10 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 1] Epoch 11 | Test AUROC 0.9998 | AUPRC 0.9998\n",
      "[Phase 1] Epoch 12 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 1] Epoch 13 | Test AUROC 0.9999 | AUPRC 0.9998\n",
      "[Phase 1] Epoch 14 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 1] Epoch 15 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 1] Epoch 16 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 1] Epoch 17 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 1] Epoch 18 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 1] Epoch 19 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 1] Epoch 20 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 2] Epoch 01 | Test AUROC 0.9991 | AUPRC 0.9984\n",
      "[Phase 2] Epoch 02 | Test AUROC 0.9990 | AUPRC 0.9983\n",
      "[Phase 2] Epoch 03 | Test AUROC 0.9995 | AUPRC 0.9993\n",
      "[Phase 2] Epoch 04 | Test AUROC 0.9994 | AUPRC 0.9991\n",
      "[Phase 2] Epoch 05 | Test AUROC 0.9991 | AUPRC 0.9985\n",
      "[Phase 2] Epoch 06 | Test AUROC 0.9995 | AUPRC 0.9993\n",
      "[Phase 2] Epoch 07 | Test AUROC 0.9997 | AUPRC 0.9996\n",
      "[Phase 2] Epoch 08 | Test AUROC 0.9994 | AUPRC 0.9991\n",
      "[Phase 2] Epoch 09 | Test AUROC 0.9997 | AUPRC 0.9997\n",
      "[Phase 2] Epoch 10 | Test AUROC 0.9997 | AUPRC 0.9996\n",
      "[Phase 2] Epoch 11 | Test AUROC 0.9998 | AUPRC 0.9997\n",
      "[Phase 2] Epoch 12 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 2] Epoch 13 | Test AUROC 0.9998 | AUPRC 0.9998\n",
      "[Phase 2] Epoch 14 | Test AUROC 0.9997 | AUPRC 0.9996\n",
      "[Phase 2] Epoch 15 | Test AUROC 0.9998 | AUPRC 0.9998\n",
      "[Phase 2] Epoch 16 | Test AUROC 0.9998 | AUPRC 0.9998\n",
      "[Phase 2] Epoch 17 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 2] Epoch 18 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 2] Epoch 19 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "[Phase 2] Epoch 20 | Test AUROC 0.9999 | AUPRC 0.9999\n",
      "===== Final Results =====\n",
      "Phase 1 (TS only)  -> AUROC: 0.9999 | AUPRC: 0.9999\n",
      "Phase 2 (TS+Profile)-> AUROC: 0.9999 | AUPRC: 0.9999\n",
      "| Model | AUROC | AUPRC |\n",
      "|-------|-------|-------|\n",
      "| TS only | 0.9999 | 0.9999 |\n",
      "| TS + Profile | 0.9999 | 0.9999 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Paths y carga base\n",
    "# =========================================\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Rutas\n",
    "DATA_DIR = Path('/home/gmartinez/Tesis/Datasets/Synthetic-data/outputs')\n",
    "HR_CSV = DATA_DIR / \"hr_series.csv\"\n",
    "SLEEP_CSV = DATA_DIR / \"sleep_series.csv\"\n",
    "LABELS_CSV = DATA_DIR / \"nightly_labeled.csv\"\n",
    "PROFILES_CSV = DATA_DIR / \"user_profiles.csv\"\n",
    "\n",
    "# Semilla y dispositivo\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# =========================================\n",
    "# Carga de datos de series y etiquetas\n",
    "# =========================================\n",
    "hr_data = pd.read_csv(HR_CSV, parse_dates=['timestamp'])\n",
    "sleep_data = pd.read_csv(SLEEP_CSV, parse_dates=['night_date'])\n",
    "labels = pd.read_csv(LABELS_CSV, parse_dates=['night_date'])\n",
    "\n",
    "hr_data['user_id'] = hr_data['user_id'].astype(str)\n",
    "sleep_data['user_id'] = sleep_data['user_id'].astype(str)\n",
    "labels['user_id'] = labels['user_id'].astype(str)\n",
    "\n",
    "# Serie diaria HR (conteo y mediana opcional)\n",
    "hr_data['date'] = hr_data['timestamp'].dt.floor('D')\n",
    "hr_daily = (\n",
    "    hr_data\n",
    "    .groupby(['user_id','date'], as_index=False)\n",
    "    .agg(hr_count=('hr','count'),\n",
    "         hr_med=('hr','median'))\n",
    ")\n",
    "\n",
    "# Etiquetas binarizadas\n",
    "label_map = {'Adequate Recovery': 1, 'Deficient Recovery': 0}\n",
    "# Ajusta el nombre real de la columna; en tu fragmento pusiste 'recovery_label_binary'\n",
    "labels['target'] = labels['recovery_label_binary'].map(label_map).astype(int)\n",
    "nights = labels[['user_id','night_date','target']].dropna().copy()\n",
    "nights['night_date'] = pd.to_datetime(nights['night_date'])\n",
    "\n",
    "# =========================================\n",
    "# Extracción de ventanas HR y features de Sleep\n",
    "# =========================================\n",
    "from datetime import timedelta\n",
    "\n",
    "SLEEP_FEATURES = ['total_hours','deep_minutes','rem_minutes','awake_minutes',\n",
    "                  'sleep_latency_minutes','awakenings','failure_minutes']\n",
    "\n",
    "def extract_hr_window(hr_daily_user, night_date, past_days=7, include_night=True, feature='hr_count'):\n",
    "    L = past_days + (1 if include_night else 0)\n",
    "    start_date = night_date - timedelta(days=past_days)\n",
    "    end_date = night_date if include_night else (night_date - timedelta(days=1))\n",
    "    idx_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    ser = (\n",
    "        pd.DataFrame({'date': idx_days})\n",
    "        .merge(hr_daily_user[['date', feature]], on='date', how='left')\n",
    "        [feature]\n",
    "        .to_numpy(dtype=float)\n",
    "    )\n",
    "    # No imputes aún; deja NaN para que el M maneje ausencias\n",
    "    if len(ser) != L:\n",
    "        ser = np.resize(ser, L)\n",
    "    return ser\n",
    "\n",
    "def extract_sleep_features_for_night(sleep_user, night_date, features=SLEEP_FEATURES):\n",
    "    row = sleep_user.loc[sleep_user['night_date'] == night_date, features]\n",
    "    if row.empty:\n",
    "        return np.array([np.nan]*len(features), dtype=float)\n",
    "    return row.iloc[0].astype(float).to_numpy()\n",
    "\n",
    "def build_samples(hr_daily, sleep_data, nights, past_days=7, use_hr_feature='hr_count'):\n",
    "    hr_by_user = {u: df.sort_values('date') for u, df in hr_daily.groupby('user_id')}\n",
    "    sleep_by_user = {u: df.sort_values('night_date') for u, df in sleep_data.groupby('user_id')}\n",
    "\n",
    "    X_list, T_list, M_list, y_list, meta_list = [], [], [], [], []\n",
    "\n",
    "    for _, row in nights.iterrows():\n",
    "        u = row['user_id']; night = row['night_date']; target = row['target']\n",
    "        hr_user = hr_by_user.get(u, pd.DataFrame(columns=['date',use_hr_feature]))\n",
    "        sleep_user = sleep_by_user.get(u, pd.DataFrame(columns=['night_date'] + SLEEP_FEATURES))\n",
    "\n",
    "        # HR canal 0\n",
    "        hr_vec = extract_hr_window(hr_user, night_date=night, past_days=past_days,\n",
    "                                   include_night=True, feature=use_hr_feature)  # (L1,)\n",
    "        L1 = len(hr_vec)\n",
    "        t_hr = np.linspace(0.0, 1.0, L1)\n",
    "        m_hr = (~np.isnan(hr_vec)).astype(float)\n",
    "\n",
    "        # Sleep en el último instante\n",
    "        s_vec = extract_sleep_features_for_night(sleep_user, night, features=SLEEP_FEATURES)  # (C2,)\n",
    "        L2 = 1\n",
    "        t_sl = np.array([1.0])\n",
    "        m_sl = (~np.isnan(s_vec)).astype(float)\n",
    "\n",
    "        t_seq = np.concatenate([t_hr, t_sl], axis=0)  # (L1+1,)\n",
    "        C = 1 + len(SLEEP_FEATURES)\n",
    "        X = np.zeros((L1+1, C), dtype=float)\n",
    "        M = np.zeros((L1+1, C), dtype=float)\n",
    "\n",
    "        # En X, conserva NaN para canales correspondientes; imputamos solo al final del z-score\n",
    "        X[:L1, 0] = hr_vec\n",
    "        M[:L1, 0] = m_hr\n",
    "\n",
    "        X[L1, 1:] = s_vec\n",
    "        M[L1, 1:] = m_sl\n",
    "\n",
    "        X_list.append(X); T_list.append(t_seq); M_list.append(M)\n",
    "        y_list.append(target); meta_list.append((u, night))\n",
    "\n",
    "    return X_list, T_list, M_list, np.array(y_list, dtype=int), meta_list\n",
    "\n",
    "X_list, T_list, M_list, y, meta = build_samples(hr_daily, sleep_data, nights, past_days=7, use_hr_feature='hr_count')\n",
    "\n",
    "# =========================================\n",
    "# Apilado y estandarización por canal (solo train)\n",
    "# =========================================\n",
    "def stack_fixed_length(X_list, T_list, M_list):\n",
    "    L = X_list[0].shape[0]; C = X_list[0].shape[1]\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    T = np.stack(T_list, axis=0)\n",
    "    M = np.stack(M_list, axis=0)\n",
    "    return X, T, M\n",
    "\n",
    "X_all, T_all, M_all = stack_fixed_length(X_list, T_list, M_list)\n",
    "\n",
    "# Split por usuario\n",
    "users = np.array([u for u, _ in meta]).astype(str)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "train_idx, test_idx = next(gss.split(np.zeros(len(users)), groups=users))\n",
    "\n",
    "# Stats por canal usando solo observados en train\n",
    "obs_mask = M_all[train_idx].astype(bool)\n",
    "sum_x = np.nansum(np.where(obs_mask, X_all[train_idx], np.nan), axis=(0,1))\n",
    "count_x = obs_mask.sum(axis=(0,1)).clip(min=1)\n",
    "mean_x = sum_x / count_x\n",
    "\n",
    "sum_sq = np.nansum(((np.where(obs_mask, X_all[train_idx], mean_x)) - mean_x)**2, axis=(0,1))\n",
    "var_x = sum_sq / count_x\n",
    "std_x = np.sqrt(np.maximum(var_x, 1e-8))\n",
    "\n",
    "def apply_standardize(X, M, mean_x, std_x):\n",
    "    Xz = (np.where(M>0, X, mean_x) - mean_x) / std_x\n",
    "    # Donde M=0, deja 0 tras z-score (neutral); el backbone usa M para ignorar esos puntos\n",
    "    Xz = np.where(M>0, Xz, 0.0)\n",
    "    return Xz\n",
    "\n",
    "X_std = apply_standardize(X_all, M_all, mean_x, std_x)\n",
    "\n",
    "X_train, T_train, M_train, y_train = X_std[train_idx], T_all[train_idx], M_all[train_idx], y[train_idx]\n",
    "X_test,  T_test,  M_test,  y_test  = X_std[test_idx],  T_all[test_idx],  M_all[test_idx],  y[test_idx]\n",
    "\n",
    "# =========================================\n",
    "# Modelo mTAN (Fase 1) y Dataset\n",
    "# =========================================\n",
    "class TimeAwareAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_model, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dk = d_model // n_heads\n",
    "        assert d_model % n_heads == 0\n",
    "        self.Wq = nn.Linear(d_in, d_model)\n",
    "        self.Wk = nn.Linear(d_in, d_model)\n",
    "        self.Wv = nn.Linear(d_in, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.time_decay = nn.Parameter(torch.tensor(1.0))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, T, M):\n",
    "        B, L, Cin = X.shape\n",
    "        Q = self.Wq(X); K = self.Wk(X); V = self.Wv(X)\n",
    "\n",
    "        def split(Z): return Z.view(B, L, self.n_heads, self.dk).transpose(1,2)\n",
    "        Qh, Kh, Vh = split(Q), split(K), split(V)\n",
    "\n",
    "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / np.sqrt(self.dk)\n",
    "        Ti = T.unsqueeze(1).unsqueeze(-1)\n",
    "        Tj = T.unsqueeze(1).unsqueeze(-2)\n",
    "        dist = torch.abs(Ti - Tj)\n",
    "        scores = scores - self.time_decay.abs() * dist\n",
    "\n",
    "        time_mask = (M.sum(dim=-1) > 0).unsqueeze(1).unsqueeze(2)\n",
    "        scores = scores.masked_fill(~time_mask, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        Z = torch.matmul(attn, Vh)\n",
    "        Z = Z.transpose(1,2).contiguous().view(B, L, self.d_model)\n",
    "        return self.out(Z), attn\n",
    "\n",
    "class MTANBackbone(nn.Module):\n",
    "    def __init__(self, c_in, d_model=128, n_heads=4, n_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(c_in, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                TimeAwareAttention(d_model, d_model, n_heads=n_heads, dropout=dropout),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(d_model, d_model*2),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(d_model*2, d_model)\n",
    "                ),\n",
    "                nn.LayerNorm(d_model),\n",
    "            ]) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, X, T, M):\n",
    "        x = self.input_proj(X)\n",
    "        for attn, ln1, ff, ln2 in self.layers:\n",
    "            h, _ = attn(x, T, M)\n",
    "            x = ln1(x + h)\n",
    "            f = ff(x)\n",
    "            x = ln2(x + f)\n",
    "        time_mask = (M.sum(dim=-1) > 0).float()\n",
    "        masked_x = x * time_mask.unsqueeze(-1)\n",
    "        denom = time_mask.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "        pooled = masked_x.sum(dim=1) / denom\n",
    "        return pooled  # (B, d_model)\n",
    "\n",
    "class TSOnlyHead(nn.Module):\n",
    "    def __init__(self, d_model=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "    def forward(self, h):\n",
    "        return self.cls(h).squeeze(-1)\n",
    "\n",
    "class ProfileMLP(nn.Module):\n",
    "    def __init__(self, p_dim, d_hidden=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(p_dim, d_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, p):\n",
    "        return self.net(p)\n",
    "\n",
    "class TSPlusProfileHead(nn.Module):\n",
    "    def __init__(self, d_model=128, p_hidden=64, d_joint=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.joint = nn.Sequential(\n",
    "            nn.Linear(d_model + p_hidden, d_joint),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_joint, 1)\n",
    "        )\n",
    "    def forward(self, h_mtan, h_prof):\n",
    "        z = torch.cat([h_mtan, h_prof], dim=-1)\n",
    "        return self.joint(z).squeeze(-1)\n",
    "\n",
    "class MTANWithHeads(nn.Module):\n",
    "    def __init__(self, c_in, d_model=128, n_heads=4, n_layers=2, p_dim=None, prof_hidden=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.backbone = MTANBackbone(c_in=c_in, d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout)\n",
    "        self.ts_head = TSOnlyHead(d_model=d_model, dropout=dropout)\n",
    "        self.use_profile = p_dim is not None\n",
    "        if self.use_profile:\n",
    "            self.prof_mlp = ProfileMLP(p_dim=p_dim, d_hidden=prof_hidden, dropout=0.1)\n",
    "            self.ts_p_head = TSPlusProfileHead(d_model=d_model, p_hidden=prof_hidden, d_joint=d_model, dropout=dropout)\n",
    "\n",
    "    def forward_phase1(self, X, T, M):\n",
    "        h = self.backbone(X, T, M)\n",
    "        return self.ts_head(h)\n",
    "\n",
    "    def forward_phase2(self, X, T, M, p):\n",
    "        h = self.backbone(X, T, M)\n",
    "        h_p = self.prof_mlp(p)\n",
    "        return self.ts_p_head(h, h_p)\n",
    "\n",
    "class TimeDataset(Dataset):\n",
    "    def __init__(self, X, T, M, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.T = torch.tensor(T, dtype=torch.float32)\n",
    "        self.M = torch.tensor(M, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.T[idx], self.M[idx], self.y[idx]\n",
    "\n",
    "train_loader_p1 = DataLoader(TimeDataset(X_train, T_train, M_train, y_train), batch_size=128, shuffle=True)\n",
    "test_loader_p1  = DataLoader(TimeDataset(X_test,  T_test,  M_test,  y_test),  batch_size=256, shuffle=False)\n",
    "\n",
    "# =========================================\n",
    "# Entrenamiento y evaluación Fase 1\n",
    "# =========================================\n",
    "def evaluate_phase1(model, loader):\n",
    "    model.eval(); ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, Tb, Mb, yb in loader:\n",
    "            Xb, Tb, Mb, yb = Xb.to(device), Tb.to(device), Mb.to(device), yb.to(device)\n",
    "            prob = torch.sigmoid(model.forward_phase1(Xb, Tb, Mb))\n",
    "            ys.append(yb.detach().cpu().numpy()); ps.append(prob.detach().cpu().numpy())\n",
    "    y_true = np.concatenate(ys); y_prob = np.concatenate(ps)\n",
    "    auroc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
    "    auprc = average_precision_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
    "    return auroc, auprc\n",
    "\n",
    "EPOCHS = 20\n",
    "set_seed(SEED)\n",
    "model_p1 = MTANWithHeads(c_in=X_train.shape[2], d_model=128, n_heads=4, n_layers=2, p_dim=None, prof_hidden=64, dropout=0.2).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model_p1.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "best = {'auroc': -1, 'state': None}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model_p1.train()\n",
    "    for Xb, Tb, Mb, yb in train_loader_p1:\n",
    "        Xb, Tb, Mb, yb = Xb.to(device), Tb.to(device), Mb.to(device), yb.to(device)\n",
    "        logit = model_p1.forward_phase1(Xb, Tb, Mb)\n",
    "        loss = criterion(logit, yb)\n",
    "        optimizer.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_p1.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    auroc, auprc = evaluate_phase1(model_p1, test_loader_p1)\n",
    "    if auroc > best['auroc']:\n",
    "        best['auroc'] = auroc\n",
    "        best['state'] = {k: v.detach().cpu().clone() for k, v in model_p1.state_dict().items()}\n",
    "    print(f\"[Phase 1] Epoch {epoch:02d} | Test AUROC {auroc:.4f} | AUPRC {auprc:.4f}\")\n",
    "\n",
    "if best['state'] is not None:\n",
    "    model_p1.load_state_dict(best['state'])\n",
    "auroc_p1, auprc_p1 = evaluate_phase1(model_p1, test_loader_p1)\n",
    "\n",
    "# =========================================\n",
    "# Perfiles: preparación desde PROFILES_CSV\n",
    "# =========================================\n",
    "# CSV con columnas: user_id, age_group, gender, physical_activity_level, smoking_status,\n",
    "# alcohol_consumption, diabetes, hypertension, age\n",
    "\n",
    "profiles_df = pd.read_csv(PROFILES_CSV)\n",
    "profiles_df['user_id'] = profiles_df['user_id'].astype(str)\n",
    "\n",
    "# Usuarios en splits\n",
    "train_users = sorted(set(users[train_idx]))\n",
    "test_users  = sorted(set(users[test_idx]))\n",
    "\n",
    "# Especificamos columnas categóricas y numéricas según tu CSV\n",
    "CAT_COLS = ['age_group','gender','physical_activity_level','smoking_status','alcohol_consumption','diabetes','hypertension']\n",
    "NUM_COLS = ['age']\n",
    "\n",
    "def fit_profile_artifacts(df_train, cat_cols, num_cols):\n",
    "    # Mapas de categorías con <UNK>\n",
    "    cat_maps = {}\n",
    "    for c in cat_cols:\n",
    "        cats = list(pd.Series(df_train[c].astype(str)).astype('category').cat.categories)\n",
    "        cat_maps[c] = cats + ([\"<UNK>\"] if \"<UNK>\" not in cats else [])\n",
    "    # Stats numéricas\n",
    "    num_stats = {}\n",
    "    for c in num_cols:\n",
    "        col = pd.to_numeric(df_train[c], errors='coerce')\n",
    "        mu = float(col.mean(skipna=True)) if col.notna().any() else 0.0\n",
    "        sd = float(col.std(skipna=True)) if col.notna().any() else 1.0\n",
    "        if sd < 1e-8: sd = 1.0\n",
    "        num_stats[c] = {\"mean\": mu, \"std\": sd}\n",
    "    return cat_maps, num_stats\n",
    "\n",
    "def one_hot_series(series, categories):\n",
    "    cat_to_idx = {cat:i for i,cat in enumerate(categories)}\n",
    "    unk = cat_to_idx.get(\"<UNK>\")\n",
    "    idx = [cat_to_idx.get(str(v), unk) for v in series.fillna(\"<UNK>\").astype(str)]\n",
    "    mat = np.eye(len(categories), dtype=np.float32)[np.array(idx, dtype=int)]\n",
    "    return mat\n",
    "\n",
    "def zscore_series(series, mean, std):\n",
    "    x = pd.to_numeric(series, errors='coerce').fillna(mean).to_numpy(dtype=np.float32)\n",
    "    return ((x - mean) / std).astype(np.float32)\n",
    "\n",
    "# Split del perfil por usuarios del train/test\n",
    "df_prof_train = profiles_df[profiles_df['user_id'].isin(train_users)].copy()\n",
    "df_prof_test  = profiles_df[profiles_df['user_id'].isin(test_users)].copy()\n",
    "\n",
    "cat_maps, num_stats = fit_profile_artifacts(df_prof_train, CAT_COLS, NUM_COLS)\n",
    "\n",
    "def build_profile_matrix(df, user_col='user_id'):\n",
    "    parts = []\n",
    "    feature_names = []\n",
    "    # num\n",
    "    for c in NUM_COLS:\n",
    "        z = zscore_series(df[c], num_stats[c]['mean'], num_stats[c]['std']).reshape(-1,1)\n",
    "        parts.append(z); feature_names.append(f\"{c}_z\")\n",
    "    # cat\n",
    "    for c in CAT_COLS:\n",
    "        oh = one_hot_series(df[c], cat_maps[c])\n",
    "        parts.append(oh); feature_names += [f\"{c}={v}\" for v in cat_maps[c]]\n",
    "    # flag presencia\n",
    "    pres = np.ones((df.shape[0],1), dtype=np.float32)\n",
    "    parts.append(pres); feature_names.append(\"profile_present\")\n",
    "    P = np.concatenate(parts, axis=1).astype(np.float32)\n",
    "    return P, feature_names\n",
    "\n",
    "P_train, profile_feature_order = build_profile_matrix(df_prof_train)\n",
    "P_test, _ = build_profile_matrix(df_prof_test)\n",
    "\n",
    "# Diccionarios user_id -> vector perfil\n",
    "prof_by_user_train = {u: v for u, v in zip(df_prof_train['user_id'].astype(str).tolist(), list(P_train))}\n",
    "prof_by_user_test  = {u: v for u, v in zip(df_prof_test['user_id'].astype(str).tolist(),  list(P_test))}\n",
    "p_dim = P_train.shape[1]\n",
    "\n",
    "# Construcción de perfiles por muestra para loaders\n",
    "def build_sample_profiles(meta, train_split_users, prof_train, prof_test, p_dim):\n",
    "    profiles = []\n",
    "    for (u, _night) in meta:\n",
    "        u = str(u)\n",
    "        if u in train_split_users:\n",
    "            vec = prof_train.get(u, None)\n",
    "        else:\n",
    "            vec = prof_test.get(u, None)\n",
    "        if vec is None:\n",
    "            # Usuario sin perfil: vector de ceros y flag 0\n",
    "            v = np.zeros((p_dim,), dtype=np.float32)\n",
    "            # Último índice es profile_present; pon 0\n",
    "            v[-1] = 0.0\n",
    "            vec = v\n",
    "        profiles.append(vec.astype(np.float32))\n",
    "    return np.stack(profiles).astype(np.float32)\n",
    "\n",
    "train_split_users_set = set(train_users)\n",
    "profiles_all = build_sample_profiles(meta, train_split_users_set, prof_by_user_train, prof_by_user_test, p_dim)\n",
    "profiles_train = profiles_all[train_idx]\n",
    "profiles_test  = profiles_all[test_idx]\n",
    "\n",
    "# =========================================\n",
    "# Fase 2: mTAN + MLP de perfil\n",
    "# =========================================\n",
    "class TimeDatasetWithProfile(Dataset):\n",
    "    def __init__(self, X, T, M, P, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.T = torch.tensor(T, dtype=torch.float32)\n",
    "        self.M = torch.tensor(M, dtype=torch.float32)\n",
    "        self.P = torch.tensor(P, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.T[idx], self.M[idx], self.P[idx], self.y[idx]\n",
    "\n",
    "train_loader_p2 = DataLoader(TimeDatasetWithProfile(X_train, T_train, M_train, profiles_train, y_train), batch_size=128, shuffle=True)\n",
    "test_loader_p2  = DataLoader(TimeDatasetWithProfile(X_test,  T_test,  M_test,  profiles_test,  y_test),  batch_size=256, shuffle=False)\n",
    "\n",
    "def evaluate_phase2(model, loader):\n",
    "    model.eval(); ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, Tb, Mb, Pb, yb in loader:\n",
    "            Xb, Tb, Mb, Pb, yb = Xb.to(device), Tb.to(device), Mb.to(device), Pb.to(device), yb.to(device)\n",
    "            prob = torch.sigmoid(model.forward_phase2(Xb, Tb, Mb, Pb))\n",
    "            ys.append(yb.detach().cpu().numpy()); ps.append(prob.detach().cpu().numpy())\n",
    "    y_true = np.concatenate(ys); y_prob = np.concatenate(ps)\n",
    "    auroc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
    "    auprc = average_precision_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
    "    return auroc, auprc\n",
    "\n",
    "# Entrenar Fase 2 (re-inicializando para comparación justa)\n",
    "set_seed(SEED)\n",
    "model_p2 = MTANWithHeads(c_in=X_train.shape[2], d_model=128, n_heads=4, n_layers=2, p_dim=p_dim, prof_hidden=64, dropout=0.2).to(device)\n",
    "criterion2 = nn.BCEWithLogitsLoss()\n",
    "optimizer2 = torch.optim.AdamW(model_p2.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer2, T_max=EPOCHS)\n",
    "best2 = {'auroc': -1, 'state': None}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model_p2.train()\n",
    "    for Xb, Tb, Mb, Pb, yb in train_loader_p2:\n",
    "        Xb, Tb, Mb, Pb, yb = Xb.to(device), Tb.to(device), Mb.to(device), Pb.to(device), yb.to(device)\n",
    "        logit = model_p2.forward_phase2(Xb, Tb, Mb, Pb)\n",
    "        loss = criterion2(logit, yb)\n",
    "        optimizer2.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_p2.parameters(), 1.0)\n",
    "        optimizer2.step()\n",
    "    scheduler2.step()\n",
    "    auroc, auprc = evaluate_phase2(model_p2, test_loader_p2)\n",
    "    if auroc > best2['auroc']:\n",
    "        best2['auroc'] = auroc\n",
    "        best2['state'] = {k: v.detach().cpu().clone() for k, v in model_p2.state_dict().items()}\n",
    "    print(f\"[Phase 2] Epoch {epoch:02d} | Test AUROC {auroc:.4f} | AUPRC {auprc:.4f}\")\n",
    "\n",
    "if best2['state'] is not None:\n",
    "    model_p2.load_state_dict(best2['state'])\n",
    "auroc_p2, auprc_p2 = evaluate_phase2(model_p2, test_loader_p2)\n",
    "\n",
    "# =========================================\n",
    "# Comparación de resultados\n",
    "# =========================================\n",
    "print(\"===== Final Results =====\")\n",
    "print(f\"Phase 1 (TS only)  -> AUROC: {auroc_p1:.4f} | AUPRC: {auprc_p1:.4f}\")\n",
    "print(f\"Phase 2 (TS+Profile)-> AUROC: {auroc_p2:.4f} | AUPRC: {auprc_p2:.4f}\")\n",
    "\n",
    "# Tabla Markdown opcional (si vas a reportar)\n",
    "def format_table(auroc1, auprc1, auroc2, auprc2):\n",
    "    return (\n",
    "        \"| Model | AUROC | AUPRC |\\n\"\n",
    "        \"|-------|-------|-------|\\n\"\n",
    "        f\"| TS only | {auroc1:.4f} | {auprc1:.4f} |\\n\"\n",
    "        f\"| TS + Profile | {auroc2:.4f} | {auprc2:.4f} |\\n\"\n",
    "    )\n",
    "\n",
    "print(format_table(auroc_p1, auprc_p1, auroc_p2, auprc_p2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
